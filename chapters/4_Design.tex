\chapter{Design}
\label{chp:design}


% Gestione del GhidraMCP e modifiche a Jadx MCP
% NOTE: This chapter has been rewritten and expanded for clarity, coherence, and academic structure.
% All original comments and TODOs have been preserved.
%design si riferisce alla fase di modellazione. Nel tuo caso, può essere la pipeline/il workflow che andrai a definire per interrogare gli LLM. Non so se i prompt vadano in design o implementation, poi vedremo...forse, implementation la toglieremo del tutto

This chapter presents the conceptual design of the automated crash–triage system.  The goal is to formalise the architecture, information flow, and reasoning model that underpin the use of \glspl{llm} for vulnerability triage of crashes  in Android native libraries.  

\begin{comment}
    The present chapter focuses on the \textcolor{red}{modelling choices ???}, the high-level workflow, the prompting principles, and the integration logic required to expose reverse-engineering evidence to the \gls{llm} via the \gls{mcp}.

Following best practices in system design, the design is organised around three pillars:
\begin{enumerate}
    \item a multi-stage analysis pipeline that governs the end-to-end data flow;
    \item a prompting and agent \textcolor{red}{framework} that constrains and guides the \gls{llm}'s behaviour;
    \item a shimming layer that guarantees correct, deterministic, and protocol-compliant interaction between the \gls{llm} and external tools. \textcolor{red}{mmhh}
\end{enumerate}

The remainder of this chapter expands each of these components in detail.
\end{comment}

\section{Pipeline Architecture} %  [Pipeline and Workflow Design]
\label{sec:pipeline_design}

The triage pipeline is designed as a structured, evidence-driven reasoning process. Its objective is to enrich the raw crash artefacts produced by POIROT with code-level context, enabling the \gls{llm} to formulate grounded judgements.

Figure~\ref{fig:pipeline_overview} provides an overview of the full workflow, which integrates: (i) POIROT’s fuzzing output, (ii) static-analysis backends exposed via \glspl{mcp}, and (iii) the \gls{llm} configured with a strict meta-prompt and output schema.

\begin{figure}[!ht]
    \centering
    \scalebox{0.8}{\input{tikzpicture/pipeline_design}}
    \caption{Overview of the triage pipeline combining POIROT fuzzing, crash analysis, and \gls{mcp}-based inspections.}
    \label{fig:pipeline_overview}
\end{figure}

The pipeline consists of the following stages.

\subsection{Crash Report Parsing (\texttt{CrashSummary})}

The raw crash logs generated by POIROT contain native stack traces from which the process-termination causes and information about the JNI entry point can be extracted.
The system extracts data from POIROT’s output (Listing~\ref{lst:POIROT-output}) and, for each crash, constructs a corresponding \texttt{CrashSummary} object as follows:
\begin{itemize}
    \item \textbf{Native entry-point method}: this is achieved by recovering the path of the crash file, which contains the name of the native function associated with the failure;
    \item \textbf{\texttt{JNI\_Bridge\_Method}}: the native entry point can be used to locate the corresponding Java declaration, since its signature---as shown in Listing~\ref{lst:JNI}---allows the system to recover the Java-side method path and the exact JNI call within the \gls{apk};
    \item \textbf{Java call graph}: after obtaining the \texttt{JNI\_Bridge\_Method}, the system filters the \gls{cfg} to retain only the paths relevant to the triage of the crash.
    \item \textbf{Libraries and methods map}: a mapping in which the key is the library name and the value is the set of relevant methods. It contains only the libraries and methods required by the \gls{llm} to complete the analysis.
\end{itemize}

\subsection{Project Initialisation and Context Loading} % [Project Initialisation: Jadx and Ghidra Setup]}

Once the crashes have been normalised, the system prepares the analysis environment by loading:
\begin{itemize}
    \item The target APK into Jadx;
    \item The relevant native libraries into Ghidra.\\
    \textbf{Note}: Ghidra allows multiple files to be loaded simultaneously; however, doing so increases its start-up time and enlarges the set of libraries that the model may inspect, without necessarily providing any additional useful information. Section~\ref{chp:LibFiltering} will describe which libraries should be retained and which should be discarded.
\end{itemize}

At this point, no \gls{llm} reasoning has occurred yet.  
This stage is strictly preparatory: it ensures that the \glspl{mcp} have access to the artefacts the model may request.

\subsection{System-Level Meta-Prompt}

Before any crash is analysed, a \textit{system-level} \textit{meta-prompt} is assigned to the \gls{llm}. This prompt:

\begin{itemize}
    \item Defines its role (``vulnerability triage agent'');
    \item Specifies behavioural constraints through a guided pipeline describing the actions the agent must perform. \\
    For instance, the prompt requires the model to reason \emph{backwards} from the point of failure, starting at the native crash origin and tracing the execution path to the furthest reachable location, both in the native stack and the Java call chain;
    \item Explains the structure of the input and clarifies the meaning of some field in the user prompt;
    \item Provides the structure of the expected output, detailing the purpose of relevant fields and the required information;
    \item Informs the model about the available \glspl{mcp} and their respective scopes;
\end{itemize}

This meta-prompt acts as the governing contract for the entire session.
It is deliberately extensive to ensure consistent and predictable behaviour.

\subsection{Crash-Specific Prompting}

For each crash, a dedicated \emph{user prompt} provides the corresponding \texttt{CrashSummary}.
This object contains all the elements required by the \gls{llm} to perform the analysis, including every piece of information that can be recovered from POIROT’s crash output.
In addition to the native stack trace, the \texttt{CrashSummary} also includes the filtered \gls{cfg}, retaining only the path leading to the Java call that triggers the native code, as well as the map of libraries and methods involved in the crash.

\subsection{Tool-Mediated Evidence Retrieval} % [Tool Invocation via MCP]}. 
The model is free to decide which tool to invoke and when to invoke it.
Although the system prompt provides a suggested pipeline that the \gls{llm} may follow to retrieve information, this sequence is not strict; rather, it serves as a guideline. The agent may adaptively choose the most appropriate \gls{mcp} call depending on the evidence required to progress in the analysis.


\subsection{Evidence Integration and Final Report Generation} % [Evidence Integration and Final Report Generation]}

Once sufficient evidence has been gathered, the \gls{llm} synthesises a structured assessment, returned as a \texttt{VulnResult}. 
The output follows a constrained JSON schema:
\begin{itemize}
    \item The \textbf{vulnerability verdict} and associated \textbf{confidence score}, the confidence should reflect the degree to which the classification is considered accurate;
    \item A \textbf{classification reasons};
    \item \textbf{\gls{cwe} identifiers} and the \textbf{estimated severity};
    \item List of \textbf{implicated libraries} and \textbf{affected functions};
    \item The relevant execution paths \textbf{Java--to--native call};
    \item Set of supporting \textbf{``reasons''} grounding the decision;
    \item Structured \textbf{evidence items}, including code excerpts, function names, addresses, and explanatory notes;
    \item \textbf{Explicit} assumptions, \textbf{limitations}, and recommended \textbf{mitigation steps}.
\end{itemize}

If \texttt{is\_vulnerability} is \texttt{true}, the result also includes a \texttt{Exploit} object providing a detailed exploitability analysis:

\begin{itemize}
    \item Exploitability level (\textit{none}, \textit{theoretical}, or \textit{practical});
    \item Triggering method (mechanism required to activate the vulnerability);
    \item Environmental or permission \textbf{prerequisites};
    \item The ordered \textbf{exploitation pipeline} describing the full attack flow;
    \item Copy-and-paste-ready \textbf{PoC commands} (e.g.\ ADB or shell commands).
\end{itemize}

The final report is entirely machine-readable and encapsulates both the analytical process and the supporting evidence.  
This makes it suitable not only for human manual inspection, but also for automated downstream processing, such as filtering high-severity findings, or generating aggregated statistics across multiple applications.  

\begin{comment}

\section{Prompting Framework [Prompting Strategy]}
\label{sec:prompting_strategies}

The prompting logic combines several complementary techniques.  
The design aims to strike a balance between flexibility (letting the model autonomously explore
the codebase) and reliability (preventing hallucinations and invalid reasoning paths).

\subsection{Meta Prompting (Primary Technique) [Meta Prompting (Primary Technique)]}

Meta prompting remains the main mechanism: the \gls{llm} is anchored by a strong, global
instruction set that defines its analytical style, the triage rubric, and the reporting schema.

\subsection{Tool-Augmented Reasoning [Tool-Augmented Reasoning (PAL / Automatic Tool Use)]}

The system uses a PAL-like pattern, where the LLM:
\begin{enumerate}
    \item identifies missing information,
    \item issues a tool call via the \gls{mcp},
    \item incorporates the response into its internal chain of reasoning.
\end{enumerate}

\subsection{ReAct: Reason + Act Loops [ReAct: Reason + Act Loops]}

The design optionally leverages ReAct patterns:  
the model first \emph{thinks}, then \emph{acts}, then \emph{reasons again} based on retrieved evidence.

\subsection{Implicit Chain-of-Thought [Implicit Chain-of-Thought]}

For safety and reproducibility reasons, explicit Chain-of-Thought is not returned to the user.  
However, the prompting encourages implicit reasoning within the model, constrained by tool-based grounding.

\subsection{Prompt Chaining [Prompt Chaining]}

Complex analyses sometimes require multi-step prompting,
particularly when Java-side context must be chained with native information.

\subsection{Self-Consistency [Self-Consistency (Retry-Based)]}

The system enforces retry mechanisms for malformed JSON or inconsistent responses, improving robustness.

\subsection{Structured Output Constraints [Structured Output Constraints]}
All results must conform to the predefined Python/Pydantic models.  
This ensures referential transparency and prevents ambiguous free-text responses.

\subsection{Techniques Not Used [Techniques Not Used]}
\textcolor{red}{To be reviewed.}  
Some prompting strategies (e.g.\ temperature scaling, prompt ensembles) were intentionally excluded
to maintain determinism.

\end{comment}

\section{Shimming Layer and \gls{mcp} Integration}
\label{sec:shimming}

A \textit{shimming layer} is required to enable a \gls{llm} to interact correctly with the \gls{mcp} tool ecosystem, particularly when the underlying model does not natively support structured \textit{tool calling}.  
Open-source language models, including those deployed locally through frameworks such as Ollama\footnote{https://ollama.com/}, possess only a conceptual understanding of tool invocation. 

They cannot retrieve information directly from the \glspl{mcp}; as a result, they tend to hallucinate missing details, producing incorrect or fabricated tool responses that cannot be consumed by an \gls{mcp}-compliant environment.


% The shimming layer acts as an intermediary between the model and the protocol. 



\begin{comment}
    \subsection{Motivation}

Without a shim, a \gls{llm} would frequently violate the protocol by hallucinating tool calls, emitting free-text instead of JSON, or skipping essential inspection steps.  
Empirically, during development we observed several recurring issues:
\begin{itemize}
    \item the model inventing \texttt{"tool": "..."} calls that do not exist;
    \item responses containing both JSON and natural language in the same turn;
    \item attempts to return a final verdict without fetching the required evidence;
    \item malformed JSON objects with missing fields;
    \item routing mistakes (e.g.\ asking Jadx to decompile native functions).
\end{itemize}

These behaviours justify the need for a constraining layer capable of enforcing correctness and preventing the model from drifting outside the protocol boundaries.



\end{comment}



\subsection{Design of the Shimming Layer} % [Design of the Shimming Layer]

The shimming layer is implemented directly as an \textit{\gls{llm} agent} whose system prompt explicitly declares the available \glspl{mcp} and defines the behavioural rules that the model must follow.
In particular, the prompt specifies the only two valid response formats: either a tool invocation or a final structured answer. Every output generated by the model must therefore be a JSON object containing exactly one of these elements. This constraint creates a disciplined interaction pattern in which the model is guided to communicate with the \glspl{mcp} correctly, despite lacking native protocol support.

Through this mechanism, the shimming layer functions as a controlled execution environment: it shapes the model’s behaviour by constraining the allowed output structure, while leaving the model’s architecture and generative process unchanged. Tool invocations and final reports thus remain consistent with the expected schema.

The system prompt provides the core operational discipline:  
\begin{itemize}
    \item responses must consist of a single JSON object;
    \item the JSON must contain either a \texttt{tool} call with arguments, or a final \texttt{answer} field;
    \item no free text is permitted outside the JSON structure;
    \item only the tools explicitly declared in the agent’s configuration (system prompt) may be invoked.
\end{itemize}

When the model emits a tool call, the agent forwards it to the appropriate \gls{mcp} server; when the model returns a final answer, it is parsed into the expected schema to validate the correctness of the JSON output.


\subsection{Why \gls{mcp} Tools Are Needed}

The shimming agent provides structure, but the core analysis relies on the \glspl{mcp}.  
These tools give the model access to concrete evidence from the application under analysis.  
Without them, the \gls{llm} would lack the contextual information required for vulnerability triage

Because the information for the triage is retrieved dynamically from the target APK and its native libraries, every decision produced by the model is based on the application's artifacts. The \gls{mcp} integration therefore prevents hallucinated analysis and ensures that the final classification is traceable to concrete evidence.

\subsection{Shimming Logic: High-Level Pseudocode}

The following pseudo-prompt and pseudo-code summarises the control flow implemented by the shimming layer.

\begin{lstlisting}[caption={Pseudo-prompt of the shimming layer}]
You are a tool-using assistant that can use tools to ...
You can ONLY communicate in JSON.

Available functionalities:

< Schema of MCP's tools >

For each step, reply ONLY with a valid JSON with the proposed schema.
{"action": <tool_name>,  "args": { ... }}

After receiving the tool results, you will be asked again.
Only when explicitly instructed with "final" may you return your writeup:
{"action": "final", "result": <writeup>}

Rules:
- NEVER output text outside of JSON.
- NEVER skip directly to "final" without using a tool.
- If you are unsure of arguments, fill with placeholders.
- Your job is to call a tool on every turn until told otherwise.
\end{lstlisting}
\vfill
\begin{lstlisting}[language=pseudoCode, caption={High-level pseudo-code of the shimming layer}]
procedure ShimmingAgent(user):
    initialise model with system prompt
    initialise empty dialogue history

    loop:
        append user prompt to history
        model_output ← query model(history)

        json ← validate_and_extract_JSON(model_output)
        if json is invalid:
            prompt ← "Invalid JSON. Try again."
            continue

        if json.action == "final":
            return json.result

        result ← execute_tool(json.action, json.args)

        if result is empty:
            prompt ← "Malformed tool call. Try another."
        else:
            prompt ← "Tool Response: " + serialize(result)


\end{lstlisting}

%This pseudocode abstracts the behaviour implemented in the actual Python code while remaining architecture-neutral. It conveys the essential control flow: enforce JSON correctness, ensure proper tool invocation, route calls deterministically, and require evidence before allowing a final judgement.

\begin{comment}
    \subsection{Discussion}

The shimming layer transforms a general-purpose \gls{llm} into an evidence-driven analysis engine.  
Rather than weakening or constraining the model, the shim amplifies its reliability by aligning its behaviour with deterministic protocol rules. In practice, this hybrid architecture---LLM reasoning combined with protocol-level enforcement---provides the benefits of a classical static-analysis pipeline while retaining the adaptability and interpretative strengths of a large language model.

\end{comment}

\section{Output Structure}

The final stage of the triage pipeline produces a structured and self-contained JSON report.  
This report contains all the contextual information required to understand the environment in which the analysis was performed, the metadata of the analysed \gls{apk}, and the full set of vulnerability assessments produced for each crash detected in the native libraries.

Each report is written to: \texttt{<out-dir>/<pkg>/<JNImethod>/report.json}

Listing~\ref{lst:jsonOutput} shows an example of the final JSON output.  
%The structure is defined by the \texttt{AnalysisContainer}, \texttt{AnalysisBlock}, and \texttt{AnalysisResult}.

\paragraph{Structure overview.}
The top-level object contains a single field, \texttt{analysis}, wrapping all the relevant metadata and results:

\begin{itemize}
  \item \textbf{\texttt{analysis.tool}} (\texttt{ToolInfo})  
  Describes the environment and configuration used for the triage:
  \begin{itemize}
    \item \texttt{model\_name}: identifier of the \gls{llm} used (e.g.\ \texttt{"gemini-cli"}, \texttt{"gpt-oss:120b"}).
    \item \texttt{apk\_path}: path to the analysed APK (e.g.\ \texttt{"APKs/com.tplink.skylight/base.apk"}).
    \item \texttt{version}: internal version of the triage tool.
  \end{itemize}

  \item \textbf{\texttt{analysis.app}} (\texttt{AppMetadata})  
  Contains the application metadata extracted from Jadx:
  \begin{itemize}
    \item \texttt{app\_name}: human-readable application label.
    \item \texttt{package}: package name (application identifier).
    \item \texttt{min\_sdk}, \texttt{target\_sdk}: minimum and target Android SDK levels.
    \item \texttt{version\_name}, \texttt{version\_code}: versioning information of the analysed build.
  \end{itemize}

  \item \textbf{\texttt{analysis.analysisResults}} (\texttt{AnalysisResults})  
  A list of per-crash assessments. Each element is an \texttt{AnalysisResult} object bundling:
  \begin{itemize}
    \item \texttt{crash} (\texttt{CrashSummary}): a normalised description of the crash:
    \begin{itemize}
      \item \texttt{ProcessTermination}: crash cause as reported by the runtime..
      \item \texttt{StackTrace}: list of native frames involved in the crash.
      \item \texttt{JavaCallGraph}: Java $\rightarrow$ JNI call chain leading to the JNI bridge method.
      \item \texttt{JNIBridgeMethod}: JNI entry point associated with the crash.
      \item \texttt{JavaCallGraph}: Java call chain leading to the JNI method.
      \item \texttt{FuzzHarnessEntry}: fuzzer entry function used to drive inputs.
      \item \texttt{ProgramEntry}: process entry point.
      \item \texttt{LibMap}: native libraries involved in the crash.

    \end{itemize}

    \item \texttt{assessment} (\texttt{VulnResult}): the vulnerability triage produced by the \gls{llm}:
    \begin{itemize}
      \item \texttt{chain\_of\_thought}: a list of strings representing the step-by-step monologue that LLM thinks through before classifying.
      \item \texttt{is\_vulnerability}: boolean verdict indicating whether the crash is likely a real vulnerability.
      \item \texttt{confidence}: numerical confidence in $[0,1]$ associated with the verdict.
      \item \texttt{reasons}: short textual bullets explaining the decision (e.g. missing null-termination, out-of-bounds read).
      \item \texttt{cwe\_ids}: list of relevant CWE identifiers (e.g.\ \texttt{"CWE-125"} for out-of-bounds read).
      \item \texttt{severity}: estimated impact level (\texttt{"low"}, \texttt{"medium"}, \texttt{"high"}, \texttt{"critical"} or \texttt{null} if unknown).
      \item \texttt{affected\_libraries}: list of libraries implicated in the crash.
      \item \texttt{recommendations}: concrete mitigation or follow-up actions (e.g.\ enforcing null-termination, adding bounds checks).
      \item \texttt{assumptions}: explicit assumptions made by the model (e.g.\ nature of the input or control over certain parameters).
      \item \texttt{limitations}: known gaps in the analysis (e.g.\ partial decompilation, missing source, lack of full context).
      \item \texttt{evidence}: list of \texttt{EvidenceItem} objects, each optionally containing:
        \begin{itemize}
            \item \texttt{function}: function name relevant to the issue.
            \item \texttt{address}: code address within the library (when available).
            \item \texttt{file}: library or source file associated with the evidence.
            \item \texttt{snippet}: short decompiled excerpt or code fragment.
            \item \texttt{note}: explanation of why this snippet supports the classification
        \end{itemize}

       \item \texttt{Statistics}: basic metrics on the analysis.
        \begin{itemize}
            \item \texttt{time}: total analysis time.
            \item \texttt{llm\_requests}: number of LLM requests.
            \item \texttt{llm\_tool\_calls}: number of MCP tool calls.
            \item \texttt{input\_tokens}: tokens sent to the LLM.
            \item \texttt{output\_tokens}: tokens produced by the LLM.
        \end{itemize}
    \end{itemize}
  \end{itemize}
\end{itemize}

This schema makes the output both human-readable and machine-consumable. 