\chapter{Design}
\label{chp:design}

%design si riferisce alla fase di modellazione. Nel tuo caso, può essere la pipeline/il workflow che andrai a definire per interrogare gli LLM. Non so se i prompt vadano in design o implementation, poi vedremo...forse, implementation la toglieremo del tutto

This chapter presents the conceptual design of the automated crash–triage system.  The goal is to formalise the architecture, information flow, and reasoning model that underpin the use of \glspl{llm} for vulnerability triage of crashes  in Android native libraries.  


\section{Data Models}

A central element in the design of the crash–triage system is the definition of a set of structured data models that organise all information exchanged between components. 
Since the \gls{llm}-based analysis relies on precise, machine-readable context, every stage of the workflow, from parsing POIROT outputs, to extracting application metadata, to producing the final vulnerability assessment, depends on well-defined representations.
These models constrain how information is encoded, ensure consistency across analyses, and guide the \gls{llm} by enforcing a predictable input and output format.

In this thesis project, all relevant information is captured through a set of custom data models, \texttt{CrashSummary}, \texttt{AppMetadata}, and \texttt{VulnResult}. Their standardised structure guarantees that the \texttt{CrashSummary} provided to the \gls{llm} have always a consistent structure, divided into clearly defined fields that are explicitly explained in the system prompt and easily interpretable by the model. Likewise, the \texttt{AppMetadata} and \texttt{VulnResult} models allow the system to request a precise and schema-constrained output from the \gls{llm}, which must adhere to the predefined structure.

Instead of asking the model directly whether the app is vulnerable and why, the system instructs the \gls{llm} to populate a fixed JSON template containing all relevant fields, with strict type and content definitions, ensuring that output is both machine- and human-readable. This approach may improve \gls{llm} robustness and reduce variability in classification tasks, because schema-based prompting helps the model focus on the attributes relevant for the decision while avoiding drifting into irrelevant details.


Moreover, enforcing a fixed structure simplifies downstream processing: outputs can be exported as well-formed \texttt{JSON} files and automatically queried or aggregated across multiple applications. For example, after a full triage run, external utilities can group results by vulnerability verdict, severity level, or associated \gls{cwe} identifiers, inspect all low-confidence cases, or generate visual summaries such as the ratio of vulnerable versus benign crashes across Android versions. 

\smallskip

The crash report may also include a possible \gls{poc}, meaning that structured outputs can additionally support automated testing: external tools can more easily extract the commands suggested by the \gls{llm} and attempt to execute them for exploit verification. This capability is not guaranteed to work reliably in all cases, but a standardised output format makes such integration technically feasible and simplifies the development of external validation utilities.


The structured design therefore relies on a set of core data models:


\begin{myitemize}
    \item \textbf{CrashSummary} Represents a single crash extracted from the analysis report.\\  
    It encapsulates key fields such as the native stack frames, JNI bridge method, a reconstructed \gls{jcg}, the set of native libraries involved in the stack trace, among other fields..  
    %This model provides to the agent all the information needed for classification.

    \item \textbf{AppMetadata}  
    Aggregates APK-level information extracted from Jadx, including the application name, package name, version code and version name, and the minimum and target SDK levels.
    %This object contextualises the crash within the application environment.

    \item \textbf{VulnResult}  
    Represents the structured output of the \gls{llm} for a single crash.\\
    It exposes information such as the chain of thought, vulnerability verdict,  CWE identifiers, severity level and an optional exploitability assessment via the \texttt{Exploit} object, as well as other contextual information.
    %Strict field definitions ensure stable parsing of \gls{llm} output.

    \item \textbf{Exploit}
    Describes the exploitability of a suspected vulnerability.\\
    It contains key fields such as the exploitability rating (unknown / theoretical / practical), the required environmental prerequisites, and Proof of Concept commands, alongside additional optional elements.
    %This model enables the \gls{llm} to assess not only the presence of a vulnerability but also its operational impact.
\end{myitemize}

Additional structures are:

\begin{myitemize}
\item \textbf{EvidenceItem} stores an piece of evidence used by the \gls{llm} to justify its reasoning, such as a function name, memory address, short decompiled snippet, and an associated explanatory note.  
\item \textbf{Statistics} collects metrics on model usage (tokens, requests, wall-clock time), enabling performance profiling and cost monitoring during large-scale triage.

\end{myitemize}

\medskip

Some of these data structures will be examined in more detail throughout this chapter.

\medskip

Overall, this strict modelling architecture ensures resilience against malformed or incomplete \gls{llm} outputs and provides a deterministic structure for downstream processing, evaluation, and reproducibility.

\section{Prompt Design}\label{chp:prompt}

A central component of the system design is the structure of the prompts used to guide the \gls{llm}.  
Because the triage procedure requires multi-step reasoning, explicit interaction with \gls{mcp} tools, and the generation of JSON outputs, the quality and organisation of the prompts directly determine the reliability of the analysis.  
Well-designed prompts reduce ambiguity and also provide a stable decision-making model, ensuring consistent behaviour across different applications and crashes.

%The prompts adopted in this work follow a hybrid structure:  a \textit{system-level meta-prompt} that defines the global reasoning procedure.

\subsection{Vulnerability Triage Prompt}

The core of the system is the vulnerability triage prompt, which governs how the \gls{llm} analyses each crash and produces its final assessment.  
The prompt employs \emph{meta prompting}\cite{zhang2025metapromptingaisystems}: the system prompt defines the agent’s role, the overall reasoning procedure, the rules for interacting with \gls{mcp} tools, and the exact JSON structure that the model must produce.  
This provides the model with a stable cognitive frame and constrains its behaviour.

In addition, the prompt enforces an implicit form of \emph{Chain-of-Thought} reasoning\cite{wei2023chainofthoughtpromptingelicitsreasoning}.  
Although the internal chain of thought is not exposed in the output, the model multi-step analytical path—starting from the crashing frame, performing backward reasoning, checking reachability, and validating assumptions, before emitting the final classification. 

The full system prompt used in the implementation is reported in Appendix~\ref{chp:appendixA}.

\subsection{Structure of the Prompt}

The prompt used for vulnerability triage is organised into a set of structured components that collectively define the agent's behaviour, analysis procedure, and output constraints.  
Rather than providing ad-hoc instructions, the prompt specifies a complete operational framework that the \gls{llm} must follow when examining each crash.  
Its structure can be understood through four conceptual layers.



\paragraph{1. Role and behavioural specification.}
The prompt begins by defining the role of the agent, its objectives, and the scope of the analysis.  
It instructs the model to behave as a vulnerability analyst with access to static-analysis tools, explicitly prohibiting unsupported assumptions, unverifiable statements, and speculative reasoning.  
Clear behavioural rules constrain how the agent must reason about the crash, how it should handle missing information, and how it should justify its conclusions.

Listing \ref{lst:prompt-role} shows the corresponding section of the prompt.

\begin{lstlisting}[language={}, caption={Role and behavioural specification in the vulnerability-triage prompt.}, label={lst:prompt-role}]
You are a **senior mobile reverse-engineering & security engineer**.
You will receive one CrashEntry at a time from a JNI-fuzzing triage pipeline.
Your task is to decide whether the crash is LIKELY caused by a genuine code vulnerability
(memory safety, logic bug, or exploitable condition) or NOT.
Return ONLY a single JSON object that strictly follows the schema below.
\end{lstlisting}

\paragraph{2. Interpretation of inputs.}
The prompt describes in detail the meaning and expected use of the inputs provided for each crash: the native stack trace, the JNI bridge method, the \gls{jcg}, and the list of relevant libraries.  
The model is instructed to integrate these artefacts into a coherent execution path.  
In particular, it must reconstruct the control flow starting from the point of failure and reason backwards to identify the root cause, assessing whether the crash is plausibly reachable and whether user-controlled data may influence the faulting operation.

Listing \ref{lst:prompt-inputs} reports the exact instructions included in the prompt.

\begin{lstlisting}[language={}, caption={Input interpretation rules provided to the LLM.}, label={lst:prompt-inputs}]
You will receive the following fields:
- process_termination
- stack_trace
- java_callgraph
- app_native_function
- jni_bridge_method
- fuzz_harness_entry
- program_entry
- relevant libraries and their JNI methods

You must integrate all fields into a coherent reconstruction of the failing execution path.
\end{lstlisting}


\paragraph{3. Tool-use protocol.}
A dedicated section of the prompt specifies how the agent should interact with external tools exposed through the \gls{mcp}.  
It explains some tool calls, what information can provide, and how retrieved evidence should influence the ongoing analysis.  
%Ensuring that tool invocations are well integrated into the model’s reasoning loop.

Listing \ref{lst:prompt-tools} shows the relevant portion of the prompt.

\begin{lstlisting}[language={}, caption={Tool-use protocol and mandatory MCP exploration steps.}, label={lst:prompt-tools}]
You have **Jadx MCP** and **Ghidra MCP** and MUST use them proactively.

Exploration Rules (excerpt):
1. Resolve thunks/imports:
   - If the crash is in a wrapper, inspect the caller via `LibMap` and decompile it.
2. Cross-library search:
   - If a symbol is missing in one `.so`, check exports in other libraries (...).
3. JNI root analysis:
   - Always decompile the App Native Function (JNI entry point).
4. Java context:
   - Inspect the `jni_bridge_method` in Jadx to check argument validation.

Steps for each crash (simplified):
1. Identify the first app-level frame below allocators/sanitizers.
2. GHIDRA MCP: Decompile the function at that frame.
3. Backward data-flow: Trace arguments backwards; recurse into callers until JNI or validation.
4. JNI/Java analysis: Check how Java constructs arguments and whether they are attacker-controlled.
5. Function-pointer resolution: Resolve indirect calls; if missing, explicitly state uncertainty.
\end{lstlisting}

\paragraph{4. Structured output requirements.}
The final part of the prompt imposes strict output constraints.  
The model must produce a JSON object consistent with a predefined schema, filling fields such as the vulnerability verdict, confidence score, severity, CWE identifiers, and other.
Each field is accompanied by explicit instructions describing what constitutes a valid value and how the agent should justify its decisions.

Listing \ref{lst:prompt-output} reports the schema-oriented instructions.

\begin{lstlisting}[language={}, caption={Structured output and JSON-schema constraints.}, label={lst:prompt-output}]
Return a JSON object with:
- `chain_of_thought`: strings. Write a step-by-step internal monologue BEFORE classifying.
- `is_vulnerable`: boolean. True if the crash is a vulnerability, false otherwise
- ...

## 6a. Exploit field requirements (only when is_vulnerable=true)
When a crash is classified as a real vulnerability:
1. You MUST provide an `exploit` object with concrete, realistic details.  
2. The `exploit_pipeline` MUST describe, in 3-5 ordered steps, ...
3. `poc_commands` MUST include at least one actionable Proof-of-Concept command..

- `exploit`: null OR an object with:
    - ...

All fields must strictly adhere to the predefined schema.
\end{lstlisting}


\bigskip

Overall, this structured prompt acts as a complete analytical workflow: it defines how the \gls{llm} interprets inputs, how it retrieves additional evidence, how it reasons about the execution path, and how it produces a vulnerability assessment.



\begin{comment}
    The present chapter focuses on the \textcolor{red}{modelling choices ???}, the high-level workflow, the prompting principles, and the integration logic required to expose reverse-engineering evidence to the \gls{llm} via the \gls{mcp}.

Following best practices in system design, the design is organised around three pillars:
\begin{enumerate}
    \item a multi-stage analysis pipeline that governs the end-to-end data flow;
    \item a prompting and agent \textcolor{red}{framework} that constrains and guides the \gls{llm}'s behaviour;
    \item a shimming layer that guarantees correct, deterministic, and protocol-compliant interaction between the \gls{llm} and external tools. \textcolor{red}{mmhh}
\end{enumerate}

The remainder of this chapter expands each of these components in detail.
\end{comment}

\section{Pipeline Architecture} %  [Pipeline and Workflow Design]
\label{sec:pipeline_design}

The triage pipeline is designed as a structured, evidence-driven reasoning process. Its objective is to enrich the raw crash artefacts produced by POIROT with code-level context, enabling the \gls{llm} to formulate grounded judgements.

Figure~\ref{fig:pipeline_overview} provides an overview of the full workflow, which integrates: (i) POIROT’s fuzzing output, (ii) static-analysis backends exposed via \glspl{mcp}, and (iii) the \gls{llm} configured with a strict prompt and output schema.

\begin{figure}[!ht]
    \centering
    \scalebox{0.8}{\input{tikzpicture/pipeline_design}}
    \caption[Overview of the triage pipeline]{Overview of the triage pipeline, combining POIROT fuzzing, crash analysis, and \glsxtrshort{mcp}-based inspections.}
    \label{fig:pipeline_overview}
\end{figure}

The pipeline consists of the following stages.

\subsection{Crash Report Parsing}% (\texttt{CrashSummary})}

At the start of the pipeline, the tool retrieves the \texttt{folder2backtraces.txt} files (Listing~\ref{lst:POIROT-output}) generated by POIROT.  
These raw crash reports contain the native stack traces, from which the process termination cause and the JNI entry point can be extracted.

For each crash, the system constructs a corresponding \texttt{CrashSummary} object, which combines the information extracted from the crash report with additional data required for the triage.

The resulting \texttt{CrashSummary} includes the following elements:

\begin{itemize}
    \item \textbf{Native Stack Trace}: The raw output produced by POIROT, consisting of the sequence of native frames leading to the crash.
    \item \textbf{JNI Bridge Method}: Identified by decoding the \gls{jni} signature extracted from the crash, allowing the system to locate the corresponding Java declaration within the \gls{apk} (Listing~\ref{lst:JNI}).
    \item \textbf{\glsxtrlong{jcg}}: Obtained by filtering the FlowDroid-generated \gls{jcg}\footnote{\url{https://github.com/secure-software-engineering/FlowDroid}} to retain only the paths relevant to the crash.  
    This helps the \gls{llm} analyse the Java-side control flow more easily and increases the coverage of the application during triage.
    \item \textbf{Libraries and methods map}: Using the stack-trace information, the tool identifies which native library contains each frame involved in the crash (including the JNI bridge). This provides the \gls{llm} with a compact map of all methods in the stack trace and the \texttt{.so} file in which each one resides.
\end{itemize}


\subsection{Initialisation and Context Loading}

Once the crashes have been normalised, the system prepares the analysis environment by loading the necessary application artefacts into the static-analysis tools.  
The target APK is opened in Jadx to expose its manifest and Java classes, while the relevant native libraries are imported into Ghidra for disassembly and decompilation.

\textbf{Note.}  
Although Ghidra supports loading multiple binaries within the same project, doing so significantly increases start-up time and expands the set of libraries that the model may query, often without yielding additional useful information.  
For this reason, only a filtered subset of libraries is loaded, as described in Section~\ref{chp:LibFiltering}.

At this point no \gls{llm} reasoning has been performed yet.  
This stage is purely preparatory: it ensures that the \glspl{mcp} can access all artefacts the model may request during the triage process.


\subsection{Agent Setup}

Before any crash is analysed, the \gls{llm} agent receives a \emph{system prompt} (shown in Listing~\ref{lst:systemPrompt}), that defines its role as a vulnerability triage agent and constrains its reasoning process.  
This prompt outlines the analysis pipeline the model must follow (see Section~\ref{chp:prompt}), explains how to interpret the inputs provided in the user prompt, and specifies the structure and purpose of the JSON output.  
It also informs the agent of the available \glspl{mcp} and clarifies when tool calls should be used.  


\subsection{Crash Prompting}

For each crash, a dedicated \emph{user prompt} provides the corresponding \texttt{CrashSummary}, which contains all the information required by the \gls{llm} to perform the analysis.  
The \texttt{CrashSummary} contains the native stack trace, the filtered \gls{jcg} leading to the JNI entry point, and the mapping of libraries and methods involved in the fault.
This ensures that the model receives a concise yet complete representation of the execution path, covering both the Java and native layers relevant to the crash.

In case of multiple crash of the same method, the agent will remain the same, is only provide with the new crash to classified as a user prompt, but the agent it's not reseted, so it already known some apk/librari stracture. However, a new agent is created whenever the analysis of a new crash begins.

\subsection{Tool-Mediated Reasoning}

During the analysis, the model is free to decide which tool to invoke and when to invoke it, with the objective of retrieving the evidence required for the classification.  
The system prompt provides a pipeline for collecting information, that serves as a guideline.  
The agent can adaptively select the most appropriate \gls{mcp} call based on the evidence needed to advance its reasoning, allowing the triage process to remain flexible while still grounded in verifiable tool-assisted retrieval.


\subsection{Assessment Generation} % [Evidence Integration and Final Report Generation]}

Once sufficient evidence about the crash has been gathered, the \gls{llm} synthesises a structured assessment, returned as a \texttt{VulnResult}.  
This object follows a strictly defined schema and contains all information required to characterise the crash, justify the classification, and, when applicable, describe a plausible exploitation path.

%\paragraph{Vulnerability Classification Fields.}
The \texttt{VulnResult} begins with the core elements of the triage:

\begin{myitemize}
    \item \textbf{chain\_of\_thought}: an internal step-by-step reasoning trace produced before the final verdict.
    \item \textbf{is\_vulnerable}: the final verdict indicating whether the crash corresponds to a likely vulnerability.
    \item \textbf{confidence}: a score in the range $[0,1]$ reflecting how strongly the evidence supports the classification.
    \item \textbf{reasons}: a list of short, concrete justifications summarising the logic behind the verdict.
    \item \textbf{cwe\_ids}: identifiers of relevant \glsxtrlong{cwe}, enabling alignment with standard vulnerability taxonomies.
    \item \textbf{severity}: an estimated impact level (low, medium, high, or critical) based on the nature of the fault and the contextual evidence.
\end{myitemize}

%\paragraph{Contextual Information.}
To provide a complete picture of the issue, the output also includes:

\begin{myitemize}
    \item \textbf{affected\_libraries}: the actual native libraries involved in the crash.
    \item \textbf{call\_sequence}: the sequence of functions—Java and native—that lead to the faulting code region.
    \item \textbf{evidence}: a collection of structured \texttt{EvidenceItem} objects, each containing a function name, address, code excerpt, or decompiled snippet, together with a brief explanatory note.
    \item \textbf{recommendations}: actionable steps or fixes that could mitigate the issue.
    \item \textbf{assumptions} and \textbf{limitations}: statements that explicitly document missing information or uncertainty in the reasoning.
\end{myitemize}

%\paragraph{Exploitability Analysis.}
If \texttt{is\_vulnerability = true}, the agent also produces an \texttt{Exploit} object describing the exploitability of the issue.  
Its structure includes:

\begin{myitemize}
    \item \textbf{exploitability}: whether exploitation is unknown, theoretical, or practically achievable.
    \item \textbf{trigger\_method}: the mechanism required to activate the vulnerability (e.g.\ malformed input, crafted intent).
    \item \textbf{prerequisites}: environmental or permission-related conditions needed for the exploit to succeed.
    \item \textbf{exploit\_pipeline}: an ordered, high-level description of how an attacker could progress from initial conditions to triggering the vulnerable behaviour.
    \item \textbf{PoC\_commands}: a theoretical, ready-to-use ADB or shell commands for reproducing the crash or exploit.
    \item \textbf{poc\_files}: references to crafted payload files used during exploitation.
\end{myitemize}

%\paragraph{Purpose of Chain-of-Thought and Chain-of-Process.}
The additional components, \textbf{Chain-of-Thought (CoT)} and the \textbf{Exploit / Chain-of-Process (CoP)} block, were included to guide the \gls{llm} toward grounded, non-speculative reasoning.

The \textbf{CoT} field stores the model's internal reasoning steps used \emph{before} classification.  
Its purpose is not to be shown to a human evaluator, but to keep the model focused on systematic, step-by-step analysis rather than relying on pattern-matching shortcuts.  
By prompting the model to reason explicitly, the system reduces the likelihood of superficial or hallucinated assessments and encourages the analysis of concrete evidence retrieved via \gls{mcp} tools. % \textcolor{red}{non so se sia veramente così e non ho dei dati, ma mi è apparso così da quando l'ho aggiunta}
Empirically, the inclusion of \gls{cot} led to more consistent and accurate classifications, reducing false positives.


The \textbf{CoP}/\texttt{Exploit} section is designed for a similar reason: it prevents the model from producing arbitrary or unrealistic exploitation scenarios.  
Instead of guessing, the model must construct a plausible and evidence-driven exploitation flow grounded in the actual structure of the application, the available attack surface, and the retrieved code snippets.  
Requiring a exploitation pipeline and concrete \gls{poc} commands constrains the model to operate within the boundaries of what is technically justified by the evidence.

%\medskip

%The final report is therefore fully machine-readable and suitable for downstream analysis, aggregation, or integration into automated pipelines, while remaining traceable and explainable thanks to the structured evidence and reasoning fields.



\begin{comment}
    
Once sufficient evidence about the crash has been gathered, the \gls{llm} synthesises a structured assessment, returned as a \texttt{VulnResult}. 
The output follows a constrained JSON schema:
\begin{myitemize}
    \item The \textbf{vulnerability verdict} (\texttt{is\_vulnerability} = True or False) and associated \textbf{confidence score}, the confidence should reflect the degree to which the classification is considered accurate;
    \item A \textbf{classification reasons};
    \item \textbf{\gls{cwe} identifiers} and the \textbf{estimated severity};
    \item List of \textbf{implicated libraries} and \textbf{affected functions};
    \item The relevant execution paths \textbf{Java--to--native call};
    \item Set of supporting \textbf{``reasons''} grounding the decision;
    \item Structured \textbf{evidence items}, including code excerpts, function names, addresses, and explanatory notes;
    \item \textbf{Explicit} assumptions, \textbf{limitations}, and recommended \textbf{mitigation steps}.
\end{myitemize}

If \texttt{is\_vulnerability} is \texttt{true}, the result also includes a \texttt{Exploit} object providing a detailed exploitability analysis:

\begin{myitemize}
    \item Exploitability level (\textit{none}, \textit{theoretical}, or \textit{practical});
    \item Triggering method (mechanism required to activate the vulnerability);
    \item Environmental or permission \textbf{prerequisites};
    \item The ordered \textbf{exploitation pipeline} describing the full attack flow;
    \item Copy-and-paste-ready \textbf{PoC commands} (e.g.\ ADB or shell commands).
\end{myitemize}

The final report is entirely machine-readable and encapsulates both the analytical process and the supporting evidence.  
This makes it suitable not only for human manual inspection, but also for automated downstream processing, such as filtering high-severity findings, or generating aggregated statistics across multiple applications.  


\end{comment}

\subsection{Final Report Generation}

For each crash of application method, the system constructs a \texttt{CrashSummary} that is passed to the \gls{llm}, enabling a contextualised classification through the Jadx and Ghidra \glspl{mcp}.  
The resulting \texttt{VulnResult}, together with the corresponding \texttt{CrashSummary} and \texttt{Statistics} for that crash, is then wrapped into an \texttt{AnalysisResult}.  
A collection of these objects forms an \texttt{AnalysisResults}, which represents all crash-level assessments associated with the same method.

Finally, the analysis run is encapsulated in an \texttt{AnalysisBlock}, which aggregates:
\begin{itemize}
    \item the tool-level information (e.g.\ selected \gls{llm} model and tool versions).
    \item the application \texttt{AppMetadata},
    \item the complete set of \texttt{AnalysisResults} for the application,
\end{itemize}

\section{Output Structure}

The \texttt{AnalysisBlock} is serialised into a well-formed \texttt{JSON} file, allowing the entire workflow, from crash extraction to final assessment, to be exported, archived, and automatically processed by external systems.  
This enables downstream tasks such as aggregating results across applications, filtering by severity or \gls{cwe} category, computing statistics, or integrating the outputs into broader testing and validation pipelines.
\medskip

\noindent Each report is written to: \texttt{<out-dir>/<pkg>/<JNImethod>/report.json}.
\noindent Listing~\ref{lst:jsonOutput} shows an example of a real final JSON output.  
%The structure is defined by the \texttt{AnalysisContainer}, \texttt{AnalysisBlock}, and \texttt{AnalysisResult}.

The top-level object contains a single field, \texttt{analysis}, wrapping all the relevant metadata and results:

\begin{itemize}
  \item \textbf{\texttt{analysis.tool}} (\texttt{ToolInfo})  
  Describes the environment and configuration used for the triage:
  \begin{myitemize}
    \item \texttt{model\_name}: identifier of the \gls{llm} used (e.g.\ \texttt{"gpt-5.1"}, \texttt{"gpt-oss:120b"}).
    \item \texttt{apk\_path}: path to the analysed APK (e.g.\ \texttt{"APKs/com.tplink.skylight/base.apk"}).
    \item \texttt{version}: internal version of the triage tool.
  \end{myitemize}

  \item \textbf{\texttt{analysis.app}} (\texttt{AppMetadata})  
  Contains the application metadata extracted from Jadx:
  \begin{myitemize}
    \item \texttt{app\_name}: human-readable application label.
    \item \texttt{package}: package name (application identifier).
    \item \texttt{min\_sdk}, \texttt{target\_sdk}: minimum and target Android SDK levels.
    \item \texttt{version\_name}, \texttt{version\_code}: versioning information of the analysed build.
  \end{myitemize}

  \item \textbf{\texttt{analysis.analysisResults}} (\texttt{AnalysisResults})  
  A \emph{list} of per-crash assessments. Each element is an \texttt{AnalysisResult} object bundling:
  \begin{itemize}
    \item \texttt{crash} (\texttt{CrashSummary}): a normalised description of the crash:
    \begin{myitemize}
      \item \texttt{ProcessTermination}: crash cause as reported by the runtime..
      \item \texttt{StackTrace}: list of native frames involved in the crash.
      \item \texttt{JavaCallGraph}: Java $\rightarrow$ JNI call chain leading to the JNI bridge method.
      \item \texttt{JNIBridgeMethod}: JNI entry point associated with the crash.
      \item \texttt{JavaCallGraph}: Java call chain leading to the JNI method.
      \item \texttt{FuzzHarnessEntry}: fuzzer entry function used to drive inputs.
      \item \texttt{ProgramEntry}: process entry point.
      \item \texttt{LibMap}: native libraries involved in the crash.

    \end{myitemize}

    \item \texttt{assessment} (\texttt{VulnResult}): the vulnerability triage produced by the \gls{llm}:
    \begin{myitemize}
      \item \texttt{chain\_of\_thought}: a list of strings representing the step-by-step monologue that LLM thinks through before classifying.
      \item \texttt{is\_vulnerable}: boolean verdict indicating whether the crash is likely a real vulnerability.
      \item \texttt{confidence}: numerical confidence in $[0,1]$ associated with the verdict.
      \item \texttt{reasons}: short textual bullets explaining the decision (e.g. missing null-termination, out-of-bounds read).
      \item \texttt{cwe\_ids}: list of relevant CWE identifiers (e.g.\ \texttt{"CWE-125"} for out-of-bounds read).
      \item \texttt{severity}: estimated impact level (\texttt{"low"}, \texttt{"medium"}, \texttt{"high"}, \texttt{"critical"} or \texttt{null} if unknown).
      \item \texttt{affected\_libraries}: list of libraries implicated in the crash.
      \item \texttt{recommendations}: concrete mitigation or follow-up actions (e.g.\ enforcing null-termination, adding bounds checks).
      \item \texttt{assumptions}: explicit assumptions made by the model (e.g.\ nature of the input or control over certain parameters).
      \item \texttt{limitations}: known gaps in the analysis (e.g.\ partial decompilation, missing source, lack of full context).
      \item \texttt{evidence}: list of \texttt{EvidenceItem} objects, each optionally containing:
        \begin{myitemize}
            \item \texttt{function}: function name relevant to the issue.
            \item \texttt{address}: code address within the library (when available).
            \item \texttt{file}: library or source file associated with the evidence.
            \item \texttt{snippet}: short decompiled excerpt or code fragment.
            \item \texttt{note}: explanation of why this snippet supports the classification
        \end{myitemize}
    \end{myitemize}
   \item \texttt{Statistics}: basic metrics on the analysis.
    \begin{myitemize}
        \item \texttt{time}: total analysis time.
        \item \texttt{llm\_requests}: number of LLM requests.
        \item \texttt{llm\_tool\_calls}: number of MCP tool calls.
        \item \texttt{input\_tokens}: tokens sent to the LLM.
        \item \texttt{output\_tokens}: tokens produced by the LLM.
    \end{myitemize}
  \end{itemize}
\end{itemize}

\begin{comment}

\section{Prompting Framework [Prompting Strategy]}
\label{sec:prompting_strategies}

The prompting logic combines several complementary techniques.  
The design aims to strike a balance between flexibility (letting the model autonomously explore
the codebase) and reliability (preventing hallucinations and invalid reasoning paths).

\subsection{Meta Prompting (Primary Technique) [Meta Prompting (Primary Technique)]}

Meta prompting remains the main mechanism: the \gls{llm} is anchored by a strong, global
instruction set that defines its analytical style, the triage rubric, and the reporting schema.

\subsection{Tool-Augmented Reasoning [Tool-Augmented Reasoning (PAL / Automatic Tool Use)]}

The system uses a PAL-like pattern, where the LLM:
\begin{enumerate}
    \item identifies missing information,
    \item issues a tool call via the \gls{mcp},
    \item incorporates the response into its internal chain of reasoning.
\end{enumerate}

\subsection{ReAct: Reason + Act Loops [ReAct: Reason + Act Loops]}

The design optionally leverages ReAct patterns:  
the model first \emph{thinks}, then \emph{acts}, then \emph{reasons again} based on retrieved evidence.

\subsection{Implicit Chain-of-Thought [Implicit Chain-of-Thought]}

For safety and reproducibility reasons, explicit Chain-of-Thought is not returned to the user.  
However, the prompting encourages implicit reasoning within the model, constrained by tool-based grounding.

\subsection{Prompt Chaining [Prompt Chaining]}

Complex analyses sometimes require multi-step prompting,
particularly when Java-side context must be chained with native information.

\subsection{Self-Consistency [Self-Consistency (Retry-Based)]}

The system enforces retry mechanisms for malformed JSON or inconsistent responses, improving robustness.

\subsection{Structured Output Constraints [Structured Output Constraints]}
All results must conform to the predefined Python/Pydantic models.  
This ensures referential transparency and prevents ambiguous free-text responses.

\subsection{Techniques Not Used [Techniques Not Used]}
\textcolor{red}{To be reviewed.}  
Some prompting strategies (e.g.\ temperature scaling, prompt ensembles) were intentionally excluded
to maintain determinism.

\end{comment}
