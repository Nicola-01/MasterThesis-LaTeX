\chapter{Implementation}

% NOTE FOR REVIEWER:
% This chapter has been rewritten and expanded, preserving all TODOs,
% comments, and \textcolor{red}{...} sections as required.
% The content now aligns with the conceptual design from Chapter 5.

The implementation realises the design described in Chapter~\ref{chp:design} through a Python-based orchestration layer built around three core elements: (i) structured data models, (ii) explicit adapters for tool interaction via the \gls{mcp}, and (iii) specialised prompting templates to ensure reproducible and deterministic behaviour across crash analyses. Also describes the concrete system architecture, library dependencies, internal module structure, and the integration workflow between the \gls{llm}, Jadx/Ghidra \glspl{mcp}, and POIROT’s crash artefacts.
% Where relevant, we also document operational constraints, practical issues encountered during development, and the rationale behind critical implementation decisions.

\section{Libraries and Dependencies}

%The implementation is intentionally lightweight, relying on a minimal set of Python libraries and external tools. This design choice ensures portability, reproducibility, and compatibility with container-based execution environments.
Main libraries and tools adopted in this thesis:

\begin{itemize}
    \item \textbf{pydantic (v2.12.3)}  
    Used for strict, typed data modelling. Every component in the triage pipeline, from crash summaries to tool responses and vulnerability assessments, conforms to well-defined Pydantic classes.  
    This guarantees deterministic parsing of the LLM outputs, ensures structural validation of all intermediate artefacts, and allows lossless serialisation and deserialisation into JSON.

    \item \textbf{pydantic\_ai}  
    Provides the agent abstraction, including the \gls{mcp}
    client and the \texttt{MCPServerStdio} transport, enabling the program
    to communicate with Jadx and Ghidra \glspl{mcp} over standard I/O streams.

    \item \textbf{Jadx} and \textbf{Ghidra}  
    The reverse-engineering backends operate as standalone \glspl{mcp}
    server. Jadx exposes bytecode, manifest, and resource-level information.
    Ghidra exposes disassembly, function listings, and decompilation output.
\end{itemize}

The system also depends on the POIROT output directory produced during fuzzing.

\section{Program Structure}

The codebase is structured around clear modular boundaries,
as illustrated in Figure~\ref{fig:implementation_structure}.

\begin{figure}[!ht]
    \centering
    %\includegraphics[width=\textwidth]{implementation_structure.pdf}
    \scalebox{0.7}{\input{tikzpicture/program-structure}}
    \caption[Structure of the implementation]{Structure of the implementation, modules, data flow, and orchestration logic.}
    \label{fig:implementation_structure}
\end{figure}

\subsection*{Entry Point and Orchestration}

The main program (\texttt{main.py}) performs the following tasks:

\begin{myenumerate}
    \item parses CLI arguments for model configuration, timeout, and output path;
    \item For each apk:
    \begin{myenumerate}
    \item loads POIROT crash artefacts;
    \item instantiates the LLM triage agent through \texttt{MCPs/get\_agent.py};
    \item coordinates the full pipeline:\\
    \(\text{load app crashes} \rightarrow \text{load APK / library context} \rightarrow \text{LLM triage} \rightarrow \text{report}\)
    \item generates the structured JSON report.
    \end{myenumerate}
\end{myenumerate}

In the same directory as the main program, several utility and helper modules are provided.  
The Ghidra and Jadx helper functions manage the corresponding GUI sessions and import of APKs and native libraries into their analysis environments.


\subsection*{\gls{mcp} Helper Functions and Data Structures}

The \texttt{MCPs/} directory contains both the core data-structure definitions used throughout the triage pipeline and a set of helper functions that support \gls{mcp}-based tool interaction.  
The data models defined in this module—such as \texttt{CrashSummary}, \texttt{AppMetadata}, \texttt{VulnResult}, \texttt{Exploit}, and \texttt{AnalysisBlock}—are those described in Chapter~\ref{chp:design}.

The directory also includes helper utilities that streamline communication with the Jadx and Ghidra \glspl{mcp}.  
These modules handle the additional initialisation steps required to make the \glspl{mcp} available to the \gls{llm}, ensuring that each backend is correctly configured before the analysis begins.

\subsection*{Prompt Specialisation}

The directory \texttt{MCPs/prompts/} contains the family of specialised prompt templates used by the triage agent.  
Each prompt serves a distinct function and is tailored to the type of information the \gls{llm} must extract or analyse.

\begin{itemize}

    \item \texttt{AppMetadata\_prompt.py}  
    Defines the extraction rules and acceptable formats for retrieving application metadata from Jadx.

    \item \texttt{VulnDetection\_prompt.py}  
    Encodes the complete vulnerability-triage logic.  
    It specifies what constitutes a vulnerability, the backward data-flow reasoning process, the conditions for exploitability, and the full JSON schema required for the output.  \\
    A variant of this prompt is used when the \gls{jcg} is unavailable; in this case, the model is not required to reason about the Java-side control flow.  
    Using the standard prompt in such situations proved problematic, as models without Java context tended to treat all inputs as attacker-controlled, leading to misclassifications.

    \item \texttt{Shimming\_prompts.py}  
    Defines the system prompts used when interacting with models that lack native \gls{mcp} support.  
    The template includes placeholders into which the required tool descriptions and system instructions are dynamically injected.  
    It also enforces the appropriate output schema—either metadata extraction or vulnerability assessment—ensuring consistent and predictable behaviour across different backends.
    
\end{itemize}




%These templates operationalise the prompting strategy described in Chapter~\ref{chp:design}.

\section{\glsxtrshort{llm} and \glsxtrshort{mcp} Integration}

The system is \gls{llm}-agnostic, it is not tied to a single model but supports multiple options. This flexibility allows selection of the provider that best fits the task, easy switching between models. Ensuring that the same triage logic can operate with different \gls{llm} vendors or even fully local models. Currently, the program supports \texttt{OpenAI}, \texttt{Gemini} and local \texttt{Ollama} models, and can be extended to include additional vendors.

The integration between the \gls{llm} and the reverse-engineering backends is realised via the \texttt{pydantic\_ai} agent framework and the standardised \gls{mcp} interface.  
This section describes the mechanisms used to initialise the agent, connect to the \glspl{mcp}, and enforce protocol correctness.

\subsection{Agent Setup}
The agent is created through the function \texttt{get\_agent} defined in \texttt{MCPs/get\_agent.py}.  
This function instantiates a \texttt{pydantic\_ai.Agent} object that coordinates the chosen \gls{llm} and a list of \gls{mcp} toolsets (\texttt{MCPServerStdio}).  
The agent receives:
\begin{itemize}
    \item \emph{A system prompt}, which defines its high-level role (e.g.\ vulnerability assessor or metadata extractor) and encodes the behavioural constraints that the model must follow. This includes the JSON-only policy, the allowed response formats, and the list of tool names that the \gls{llm} may invoke.
    
    \item \emph{An output type}, such as \texttt{AppMetadata} or \texttt{VulnResult}. This schema determines how the final answer must be structured and enables automatic validation of the model’s output.
    
    \item A list of connected \emph{\gls{mcp} servers}, each represented by a \texttt{MCPServerStdio} instance. These servers expose the reverse-engineering capabilities that the agent can access whenever the model chooses to perform a tool call.

\end{itemize}

The function automatically selects the appropriate model provider according to the prefix of the model name supplied at start-up:
\begin{itemize}
    \item names beginning with \texttt{gpt-} initialise an \texttt{OpenAIChatModel} via the \texttt{OpenAIProvider};
    \item names beginning with \texttt{gemini-} initialise a \texttt{GoogleModel} via the \texttt{GoogleProvider};
    \item any other identifier defaults to a locally hosted model served through the \texttt{OllamaProvider}.
\end{itemize}
This logic allows seamless switching between cloud-based and local models.

The agent also keeps track of the number of tool calls executed during the analysis, as well as the token usage associated with each step, both the input tokens sent to the model and the output tokens generated in response, enabling detailed monitoring of the interaction.

\subsubsection{Gemini-CLI.}
If the model name is specified as \texttt{gemini-cli}, the program interacts with the Gemini Command-Line Interface\footnote{https://geminicli.com/} instead of the standard \gls{api} endpoint.
%This configuration was introduced to bypass \gls{api} usage quotas and benefit from the higher request limits available with \texttt{gemini-2.5-pro}. 

The integration is handled by the helper module \texttt{MCPs/geminiCLI.py}, which provides two key functions: \texttt{query\_gemini\_cli()} and \texttt{gemini\_response\_parser()}.
%Since the Gemini CLI does not support distinct system and user prompts or structured JSON output (as required by \texttt{pydantic\_ai.Agent}), these functions implement a custom prompting and parsing layer.
\texttt{query\_gemini\_cli()} builds a unified text prompt combining the system and user contexts, executes the \texttt{gemini} command via subprocess.
The model’s response is then passed to \texttt{gemini\_response\_parser()}, which cleans the raw \gls{cli} output and attempts to parse the result as JSON. If the output cannot be validated against the expected schema, the function retries; after four failed attempts, an error is raised.

\subsubsection{Local LLM Backends (Ollama) and the Shimming Layer}

Local LLMs, particularly open-source models run through systems such as Ollama, do not provide native support for the \gls{mcp} protocol.
When used directly, these models frequently violate the protocol by hallucinating tool calls, mixing JSON with free text, or returning incomplete or malformed outputs.  
To ensure protocol-correct behaviour, a dedicated \emph{shimming layer} is introduced.

The agent therefore instantiates an \texttt{oss\_model} backed by \texttt{MCPs/shimming\_agent.py}, which acts as a controlled execution environment.  
Instead of allowing the model to interact freely with Jadx and Ghidra, the shimming agent constrains all communication to a strict JSON structure and enforces disciplined tool use.

\paragraph{Behaviour enforced by the shim.}
The system prompt associated with the shimming agent specifies that every model response must be a single JSON object containing either:
\begin{itemize}
    \item a tool invocation, e.g.\ \verb|{"action": "<tool_name>", "args": {...}}|, or
    \item a final answer, e.g.\ \verb|{"action": "final", "result": {...}}|.
\end{itemize}

No free text is permitted outside this structure.  
When the model outputs an action, the Python wrapper executes the corresponding tool through the \texttt{BasicMCPClient} provided by the \texttt{llama\_index} MCP utilities, forwards the result back to the model, and continues the interaction loop.

Only when the model emits \texttt{"action": "final"} does the agent proceed to validating the output against the expected schema (e.g.\ \texttt{AppMetadata}, \texttt{VulnResult}).

\paragraph{Execution loop.}
During a shimming-mediated interaction:
\begin{myenumerate}
    \item the model proposes a tool call in JSON form;
    \item the agent forwards the request to the appropriate \gls{mcp} server (Jadx or Ghidra);
    \item the server returns structured evidence;
    \item the agent returns this evidence to the model as the next input;
    \item the model decides whether additional evidence is required or whether it can produce the final structured output.
\end{myenumerate}

If the final JSON object is well-formed and matches the target schema, the result is accepted; otherwise, the execution is interrupted.

\paragraph{Purpose.}
By mediating the interaction in this way, the shimming layer prevents protocol violations and ensures that local open-source models behave like fully \gls{mcp}-compatible agents.  
%It preserves the generative capabilities of the underlying \gls{llm} while guaranteeing that tool calls, evidence retrieval, and final outputs remain valid and reproducible.

% ----------------------
\begin{comment}
    

\subsubsection{Local LLM (Ollama)}
When the model name does not match the cloud prefixes (\texttt{gpt-}/\texttt{gemini-}), the agent assumes a local configuration and instantiates an \texttt{oss\_model} object backed by the \texttt{MCPs/shimming\_agent.py}. 
%As mentions in Section~\ref{sec:shimming}, some models don't have the cabability to use \gls{mcp}, e.g. open source model, so the shimming\_agent will take the action, using the \gls{llm} provided my Ollama, and extra notions provided by the system pompt, the model is able to choose the methods needed, and required it usign \verb|("action": <tool_name>, "args": ( ... ),)|, then, using BasicMCPClient of llama\_index.tools.mcp, the wrapper python code execute the call instead the model, then the response is send to the \gls{llm} as a user prompt.
%if the model have enought informations it can reply with \verb|{"action":"final","result": {...}}| with the final reply, this will check the correctness of output\_type i.e. the code will check that the moldel had reply with the required json with all the fields

\paragraph{Shimming layer.}
As discussed in Section~\ref{sec:shimming}, some models, particularly open-source ones, do not possess native support for the \gls{mcp} protocol. In these cases, the \texttt{shimming\_agent} acts as the execution layer that enables structured tool use.  
The agent runs the \gls{llm} provided by Ollama and enriches it with the behavioural rules defined in the system prompt. With these rules in place, the model can select which tool it wants to invoke and express this decision through a JSON message of the form: \verb|{"action": "<tool_name>", "args": { ... }}|.
When such an action is emitted, the Python wrapper executes the requested call on behalf of the model. This is done via the \texttt{BasicMCPClient} from \texttt{llama\_index.tools.mcp}, which sends the arguments to the corresponding \gls{mcp} server (e.g.\ Jadx or Ghidra) and collects the response.
The result returned by the \gls{mcp} server is then forwarded back to the \gls{llm} as a new user prompt. This iterative loop continues until the model determines that it has gathered sufficient evidence to produce the final structured output. 
At that point, the model returns a JSON object of the form: \verb|{"action": "final", "result": { ... }}|.

The agent then validates this object against the expected \texttt{output\_type} (e.g.\ \texttt{AppMetadata} or \texttt{VulnResult}).  
This validation ensures that: the model has produced a well-formed JSON object, all mandatory fields are present and the types match the schema.
Only when these conditions are satisfied does the agent return the final result. Otherwise, the pipeline is interrupted, signalling that the model produced an incomplete or invalid output.

%This mechanism allows models without native \gls{mcp} support to perform multi-step, tool-augmented reasoning while ensuring that all interactions remain safe, structured, and reproducible.

%This configuration enables on-premise execution through Ollama, ensuring full data isolation and allowing the entire triage pipeline to run in offline or restricted environments.


\section{Shimming Layer and \gls{mcp} Integration}
\label{sec:shimming}

A \textit{shimming layer} is required to enable a \gls{llm} to interact correctly with the \gls{mcp} tool ecosystem, particularly when the underlying model does not natively support structured \textit{tool calling}.  
Open-source language models, including those deployed locally through frameworks such as Ollama\footnote{https://ollama.com/}, possess only a conceptual understanding of tool invocation. 

They cannot retrieve information directly from the \glspl{mcp}; as a result, they tend to hallucinate missing details, producing incorrect or fabricated tool responses that cannot be consumed by an \gls{mcp}-compliant environment.


% The shimming layer acts as an intermediary between the model and the protocol. 


\end{comment}

\begin{comment}
    \subsection{Motivation}

Without a shim, a \gls{llm} would frequently violate the protocol by hallucinating tool calls, emitting free-text instead of JSON, or skipping essential inspection steps.  
Empirically, during development we observed several recurring issues:
\begin{itemize}
    \item the model inventing \texttt{"tool": "..."} calls that do not exist;
    \item responses containing both JSON and natural language in the same turn;
    \item attempts to return a final verdict without fetching the required evidence;
    \item malformed JSON objects with missing fields;
    \item routing mistakes (e.g.\ asking Jadx to decompile native functions).
\end{itemize}

These behaviours justify the need for a constraining layer capable of enforcing correctness and preventing the model from drifting outside the protocol boundaries.



\end{comment}



\begin{comment}
\subsection{Design of the Shimming Layer} % [Design of the Shimming Layer]

The shimming layer is implemented directly as an \textit{\gls{llm} agent} whose system prompt explicitly declares the available \glspl{mcp} and defines the behavioural rules that the model must follow.
In particular, the prompt specifies the only two valid response formats: either a tool invocation or a final structured answer. Every output generated by the model must therefore be a JSON object containing exactly one of these elements. This constraint creates a disciplined interaction pattern in which the model is guided to communicate with the \glspl{mcp} correctly, despite lacking native protocol support.

Through this mechanism, the shimming layer functions as a controlled execution environment: it shapes the model’s behaviour by constraining the allowed output structure, while leaving the model’s architecture and generative process unchanged. Tool invocations and final reports thus remain consistent with the expected schema.

The system prompt provides the core operational discipline:  
\begin{itemize}
    \item responses must consist of a single JSON object;
    \item the JSON must contain either a \texttt{tool} call with arguments, or a final \texttt{answer} field;
    \item no free text is permitted outside the JSON structure;
    \item only the tools explicitly declared in the agent’s configuration (system prompt) may be invoked.
\end{itemize}

When the model emits a tool call, the agent forwards it to the appropriate \gls{mcp} server; when the model returns a final answer, it is parsed into the expected schema to validate the correctness of the JSON output.

\end{comment}


\subsubsection{Shimming Logic: High-Level Pseudocode}

The following pseudo-prompt and pseudo-code summarises the control flow implemented by the shimming layer.

\begin{lstlisting}[caption={Pseudo-prompt of the shimming layer}]
You are a tool-using assistant that can use tools to ...
You can ONLY communicate in JSON.

Available functionalities:

< Schema of MCP's tools >

For each step, reply ONLY with a valid JSON with the proposed schema.
{"action": <tool_name>,  "args": { ... }}

After receiving the tool results, you will be asked again.
Only when explicitly instructed with "final" may you return your writeup:
{"action": "final", "result": <writeup>}

Rules:
- NEVER output text outside of JSON.
- NEVER skip directly to "final" without using a tool.
- If you are unsure of arguments, fill with placeholders.
- Your job is to call a tool on every turn until told otherwise.
\end{lstlisting}
\vfill
\begin{lstlisting}[language=pseudoCode, caption={High-level pseudo-code of the shimming layer}]
procedure ShimmingAgent(user):
    initialise model with system prompt
    initialise empty dialogue history

    loop:
        append user prompt to history
        model_output ← query model(history)

        json ← validate_and_extract_JSON(model_output)
        if json is invalid:
            prompt ← "Invalid JSON. Try again."
            continue

        if json.action == "final":
            return json.result

        result ← execute_tool(json.action, json.args)

        if result is empty:
            prompt ← "Malformed tool call. Try another."
        else:
            prompt ← "Tool Response: " + serialize(result)


\end{lstlisting}

%This pseudocode abstracts the behaviour implemented in the actual Python code while remaining architecture-neutral. It conveys the essential control flow: enforce JSON correctness, ensure proper tool invocation, route calls deterministically, and require evidence before allowing a final judgement.

\begin{comment}
    \subsection{Discussion}

The shimming layer transforms a general-purpose \gls{llm} into an evidence-driven analysis engine.  
Rather than weakening or constraining the model, the shim amplifies its reliability by aligning its behaviour with deterministic protocol rules. In practice, this hybrid architecture---LLM reasoning combined with protocol-level enforcement---provides the benefits of a classical static-analysis pipeline while retaining the adaptability and interpretative strengths of a large language model.

\end{comment}


\begin{comment}
\subsection{\gls{mcp} Setup}

\colorbox{red}{Jadx and Ghidra are launched as independent \glspl{mcp} servers. The Python orchestration layer connects to each server using \texttt{MCPServerStdio}, exposing a unified interface to the LLM.}

The shim layer enforces:
    
\begin{itemize}
    \item correct routing (Jadx for Java context, Ghidra for native code),
    \item one tool call per turn,
    \item error-handling and automatic retries,
    \item normalisation of malformed JSON responses,
    \item timeouts imposed via CLI parameters.
\end{itemize}
%This ensures deterministic behaviour even when using stochastic models.
\end{comment}



\subsection{\glsxtrshort{mcp} setup}

The use of \glspl{mcp} requires additional setup steps, and the exact procedure depends on the execution environment in which the \gls{llm} operates.  
Different backends, cloud-based services, the command-line \texttt{gemini-cli} interface, or local offline engines such as Ollama, require different integration strategies.  
As a result, the orchestration layer must initialise and manage the \gls{mcp} servers in a manner that is compatible with the chosen execution backend, ensuring that the \gls{llm} can reliably invoke tool operations throughout the triage process.


\subsubsection{Jadx}

The integration of Jadx into the pipeline relies on the \texttt{jadx-ai-mcp}\footnote{\url{https://github.com/zinja-coder/jadx-ai-mcp}} server, which exposes Jadx functionalities.

Unlike Ghidra, Jadx does not require any additional configuration steps once the \gls{mcp} plugin is installed. 

The server becomes available as soon as the Jadx \gls{gui} instance is running, and de designated port is free.
Therefore, when analysis of an \gls{apk} begins, the program launches a dedicated Jadx session using the command: \verb|jadx-gui <apk>| 
This opens the \gls{gui} instance and allows the \texttt{jadx-ai-mcp} plugin to start its server.  

Session management is handled by the helper module \texttt{jadx\_helper\_functions.py}, which contains utilities to start, monitor, and terminate the Jadx process.  
The function \texttt{start\_jadx\_gui()} launches the GUI, monitors its output stream, and blocks execution until the \gls{mcp} server is detected or a timeout occurs.  
If Jadx fails to start within the configured timeout, the function terminates the process and aborts the analysis.  
The module provides \texttt{kill\_jadx()}, used to gracefully stop the Jadx process when it is no longer required or when switching to a different \gls{apk}.  


\paragraph{Cloud-based backends.}
For cloud-based \gls{llm} backends (e.g.\ GPT-5.1 or Gemini-2.5), integration with \gls{mcp} servers can rely directly on the Pydantic AI framework\footnote{\url{https://docs.pydantic.dev/latest/}}.  
The \texttt{MCPServerStdio} utility provided by Pydantic AI enables the orchestration layer to launch and manage \gls{mcp} servers as subprocesses, defining both the command used to start the server and the timeout required for its initialisation.

Listing~\ref{lst:jadxServer} illustrates how the Jadx \gls{mcp} server is initialised by constructing an \texttt{MCPServerStdio} instance, which invokes \texttt{jadx\_mcp\_server.py} from the directory specified by the \texttt{JADX\_MCP\_DIR} environment variable.


\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_jadx\_server} used to initialise the Jadx \gls{mcp} connection}, label={lst:jadxServer}]
def make_jadx_server(timeout: int = 60) -> MCPServerStdio:
    return MCPServerStdio(
        "uv",
        args=["--directory", os.getenv("JADX_MCP_DIR"), "run", "jadx_mcp_server.py"],
        timeout=timeout,
    )
\end{lstlisting}



Once the \texttt{MCPServerStdio} is running, the agent connects to the \texttt{jadx-ai-mcp} endpoint to query project-level information.  
Listing~\ref{lst:getMetadata} shows how the system retrieves the \gls{apk} metadata using a dedicated agent call.

\begin{lstlisting}[language=pyMCP, caption={Method \texttt{get\_jadx\_metadata} that returns the app metadata from Jadx}, label={lst:getMetadata}]
server: MCPServerStdio = make_jadx_server()
...
async with get_agent(JADX_APP_METADATA, AppMetadata, [server], model_name=model_name) as j_agent:
    j_meta = await j_agent.run("Extract app metadata from the currently open Jadx project.")
appMetadata: AppMetadata = j_meta.output   
...
\end{lstlisting}

\paragraph{\texttt{jadx-ai-mcp} tools.}
The Jadx \gls{mcp} server provides several groups of tools that allow the agent to inspect and navigate the Java and Android components of the application:

\begin{itemize}
    \item \textbf{Manifest and Entry-Point Retrieval:}  
    Extract information from the Android manifest and identify key components such as activities, application classes, and initial entry points.

    \item \textbf{Class and Method Inspection:}  
    Explore the full set of classes, access source code, and inspect methods, enabling structural analysis of the Java layer.

    \item \textbf{Search and Low-Level Views:}  
    Locate methods by name—including JNI bridge functions—and retrieve Smali representations for low-level inspection.

    \item \textbf{Resource Access:}  
    Retrieve resources, configuration files, and embedded strings, useful for understanding app behaviour or identifying protocol- or input-related logic.

    \item \textbf{GUI-Assisted Context Capture:}  
    Interact with elements currently selected in the Jadx GUI, allowing the agent to access context-sensitive views during analysis.

    \item \textbf{Refactoring Utilities:}  
    Optional renaming tools that improve readability within Jadx without affecting the underlying binary or the evidence used for the final report.
\end{itemize}

\subsubsection{Ghidra}

The integration of Ghidra into the pipeline relies on the \texttt{GhidraMCP}\footnote{\url{https://github.com/LaurieWired/GhidraMCP}}, which exposes Ghidra’s analysis and decompilation functionalities.

Unlike Jadx, Ghidra cannot automatically create projects or import binaries directly from the command line, which makes its initial setup less straightforward.  
To address this limitation, the pipeline employs \texttt{ghidra-cli}\footnote{\url{https://github.com/Denloob/ghidra-cli}}, a wrapper script that allows the creation of a new project and the import of native libraries using a simple command such as: \verb|ghidra-cli -n -i file.so|
This command launches the Ghidra \gls{gui} (see Figure~\ref{fig:ghidraGUI}) and automatically imports the specified binaries.

\begin{figure}[H]
    \centering
    \includegraphics[width=12cm]{figures/GhidraGUI.png}
    \caption[Ghidra GUI]{Ghidra \glsxtrshort{gui} after being launched via \texttt{ghidra-cli}, with multiple .so files.}
    \label{fig:ghidraGUI}
\end{figure}

However, the Ghidra MCP server only becomes available once an imported binary is opened inside the CodeBrowser (interface opened by the dragon icon), and this action cannot be performed via the command line.

To fully automate this process, the helper module \texttt{ghidraMCP\_helper\_functions.py} was developed.  
This module uses \texttt{wmctrl} to manage desktop windows and \texttt{pyautogui} to simulate keyboard interactions (e.g.\ \texttt{Tab}, \texttt{Down Arrow}, \texttt{Enter}) in order to programmatically select and open imported binaries within the GUI.  
It also supports opening the Ghidra \gls{gui} with multiple imported files, switching between them programmatically, and closing the currently open file before loading a new one.

Since the \gls{mcp} server is bound to the active CodeBrowser instance, the \gls{llm} can only retrieve information from a single \texttt{.so} file at any given time.  
Running multiple CodeBrowser instances is not possible because they would share the same MCP port.  
However, real applications often rely on multiple native libraries, and analysing them sequentially requires explicit control over which binary is currently active in Ghidra.

To address this limitation, the base \texttt{GhidraMCP} implementation was extended with three additional methods:

\begin{myitemize}

    \item \texttt{list\_available\_libs()}:
    Returns the list of all native libraries currently imported into the Ghidra project.  
    %Allows the triage agent to discover which binaries can be analysed before issuing analysis-related tool calls.  
    %It is particularly useful for verifying whether a specific library is already available or whether a new import operation is required.
    
    \item \texttt{get\_current\_lib\_name()}:
    Identifies the library that is currently open in the Ghidra CodeBrowser and therefore exposed through the MCP server.  
    %This method ensures that analysis actions (e.g.\ decompilation, symbol lookups) are performed on the correct binary and that the context aligns with the LLM’s expectations.
    
    \item \texttt{open\_lib(lib\_name: str)}:
    Switches the active view in the CodeBrowser to the specified library.  
    %The name must match one of the entries returned by \texttt{list\_available\_libs()}.  
    This operation brings the selected binary into focus so that subsequent MCP calls operate on the intended file.

    \item \texttt{open\_external\_method(method\_name: str)}:
    Allows the agent to analyse methods that reside in libraries not currently loaded into Ghidra.
    When invoked, this function closes the active Ghidra GUI instance, identifies the native library that defines the requested method, and reopens Ghidra with that library included among the imported files.  
    This ensures that the \gls{llm} can retrieve code and metadata for methods located outside the initially loaded set of libraries, without requiring manual intervention.

\end{myitemize}

These extensions enable controlled, sequential analysis of multiple native libraries while maintaining compatibility with Ghidra’s single-CodeBrowser \gls{mcp} constraint.

%Listing~\ref{lst:ghidraServer} shows how the \gls{mcp} connection is established using the \texttt{MCPServerStdio} interface.

\begin{comment}
    
\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_ghidra\_server} used to initialise the Ghidra \gls{mcp} connection}, label={lst:ghidraServer}]
def make_ghidra_server(debug: bool = False, verbose: bool = False, timeout: int = 120) -> MCPServerStdio:
    ghidra_mcp_dir = os.getenv("GHIDRA_MCP_DIR")
    ghidra_dir = os.getenv("GHIDRA_INSTALL_DIR")
        
    return MCPServerStdio(
        "python3",
        args=[
            f"{ghidra_mcp_dir}/bridge_mcp_ghidra.py",
            "--ghidra-server", "http://127.0.0.1:8080/",
        ],
        env={"GHIDRA_INSTALL_DIR": ghidra_dir},
        timeout=timeout,
    )
\end{lstlisting}
\end{comment}


%The integration of Ghidra into the pipeline relies on the \texttt{GhidraMCP}\footnote{\url{https://github.com/LaurieWired/GhidraMCP}}, which exposes Ghidra’s analysis and decompilation capabilities through the \gls{mcp} protocol.  
%Unlike Jadx, Ghidra doesn't supports the start and import of new projects in an easy way from therminal, it have some issues, a work around was to use \texttt{ghidra-cli}\footnote{\url{https://github.com/Denloob/ghidra-cli}}, That allows to easily create new project and import binary using \gls{cli}, by using the command \verb|ghidra-cli -n -i ./my_binary|, this start the Ghidra \gls{gui} (Figure~\ref{fig:ghidraGUI}), importing the ELF binaries (\texttt{.so} files extracted from the target \gls{apk}) in a random name project, the problem for here is that there isn't a cli way to select the imported file, and the Ghidra \gls{mcp} starts only when the binary is launched with che Ghidra's CodeBrowser (the dragon Icon). To Automate this operation \texttt{ghidraMCP\_helper\_functions.py} was developed, this helper modul use \texttt{wmctrl} to switch pc windows and \texttt{pyautogui} to send keyboard signal, as `tab`, `down arrow` and `enter` to select the file automaticly from cli.

\begin{comment}

\begin{figure}
    \centering
    \includegraphics[width=12cm]{figures/GhidraGUI.jpeg}
    \caption{Ghidra \gls{gui} after being launched via \texttt{ghidra-cli}.}
    \label{fig:ghidraGUI}
\end{figure}

    Session management is entirely handled within \texttt{ghidraMCP\_helper\_functions.py}, which provides utilities to launch, monitor, and close Ghidra processes.  
The function \texttt{openGhidraGUI()} invokes \texttt{ghidra-cli} to initialise the environment, ensures no other active Ghidra instances are running, and confirms successful startup through window detection.  
If a window titled “Ghidra:” is already open, it is automatically closed before the new session begins.  
The helper then brings the \gls{gui} to the foreground and completes the import process through simulated keyboard input.  
A built-in timeout ensures that the Ghidra environment is properly initialised before the analysis continues.

\end{comment}
\hfill \break
Once the Ghidra \gls{gui} environment is ready, the triage agent establishes communication with the Ghidra \gls{mcp} server through the \texttt{make\_ghidra\_server()} function defined in \texttt{ghidraMCP.py}.  

\begin{comment}
    
Listing~\ref{lst:ghidraServer}

\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_ghidra\_server} used to initialise the Ghidra \gls{mcp} connection}, label={lst:ghidraServer}]
def make_ghidra_server(debug: bool = False, verbose: bool = False, timeout: int = 120) -> MCPServerStdio:
    ghidra_mcp_dir = os.getenv("GHIDRA_MCP_DIR")
    ghidra_dir = os.getenv("GHIDRA_INSTALL_DIR")
        
    return MCPServerStdio(
        "python3",
        args=[
            f"{ghidra_mcp_dir}/bridge_mcp_ghidra.py",
            "--ghidra-server", "http://127.0.0.1:8080/",
        ],
        env={"GHIDRA_INSTALL_DIR": ghidra_dir},
        timeout=timeout,
    )
\end{lstlisting}

Listing~\ref{lst:mcpVulnAssessment} shows the use of \texttt{jadx-ai-mcp} and \texttt{GhidraMCP} for perform vulnerability triage process. 
\end{comment}



\paragraph{\texttt{GhidraMCP} tools.}
The Ghidra \gls{mcp} server exposes several groups of analysis capabilities that the triage agent can invoke during native-code inspection:

\begin{itemize}
    \item \textbf{Decompilation and Disassembly:}  
    Provide low-level assembly views and high-level decompiled C-like code, enabling the agent to inspect function bodies and reconstruct program behaviour.

    \item \textbf{Function and Address Queries:}  
    Allow the agent to retrieve functions, addresses, and cross-references, making it possible to trace control flow and understand how different parts of the binary interact.

    \item \textbf{Listing and Enumeration:}  
    Return structured listings of functions, classes, symbols, segments, strings, and other program components, giving the agent a global overview of the binary.

    \item \textbf{Search and Cross-References:}  
    Support targeted lookups (e.g., by name or substring) and navigation of cross-references, helping the agent locate relevant code regions linked to the crash.

    \item \textbf{Editing and Annotation:}  
    Enable renaming, prototype adjustments, and commenting within the decompiler or disassembly views, improving clarity during iterative analysis.
\end{itemize}

\begin{comment}
    

\begin{myitemize}
  \item \textbf{Decompilation and Disassembly:}  
  \texttt{disassemble\_function} (return: assembly text).
  \texttt{decompile\_function}, \texttt{decompile\_function\_by\_address} (return: C code);  
  
  \item \textbf{Function and Address Queries:}  
  \texttt{get\_current\_address}, \texttt{get\_current\_function},  
  \texttt{get\_function\_by\_address} (return: name/object),  
  \texttt{get\_function\_xrefs}, \\ \texttt{get\_xrefs\_from}, \texttt{get\_xrefs\_to} (return: reference lists).
  
  \item \textbf{Listing and Enumeration:}  
  \texttt{list\_functions}, \texttt{list\_methods}, \texttt{list\_classes}, \\
  \texttt{list\_namespaces}, \texttt{list\_segments}, \texttt{list\_data\_items},  \texttt{list\_imports},  \\ 
  \texttt{list\_exports},  \texttt{list\_strings} (return: lists of names, symbols, or values).
  
  \item \textbf{Search and Cross-References:}  
  \texttt{search\_functions\_by\_name} (substring search),  \\
  \texttt{get\_function\_xrefs}, \texttt{get\_xrefs\_from}, \\
  \texttt{get\_xrefs\_to} (return: reference objects).
  
  \item \textbf{Editing and Annotation:}  \texttt{rename\_function\_by\_address}, \texttt{rename\_function}, \\
  \texttt{set\_function\_prototype}, \texttt{set\_local\_variable\_type},  \texttt{rename\_variable}, \\
\texttt{rename\_data},  \texttt{set\_decompiler\_comment}, \texttt{set\_disassembly\_comment} (return: flag or void).
\end{myitemize}

\end{comment}

\subsubsection{\gls{mcp} on Gemini-cli and Open Source \gls{llm}}

When the selected model is neither a cloud-based GPT nor Gemini model, the system falls back to a local execution strategy.  
Two pathways are implemented: one based on \texttt{gemini-cli} and one based on open-source models served through \texttt{Ollama}.  
Both approaches differ from the standard \texttt{MCPServerStdio}-based configuration.

\paragraph{Gemini-cli.}
The integration of Gemini models is handled through the standalone \texttt{gemini} command-line interface.  
Instead of establishing a persistent \gls{mcp} transport via \texttt{MCPServerStdio}, the system invokes \texttt{gemini-cli} directly; it is then \texttt{gemini-cli} itself that, based on its configuration file \texttt{.gemini/settings.json}, automatically launches the required \gls{mcp} servers.
When \texttt{gemini-cli} starts, it spawns the corresponding \gls{mcp} processes, after which the triage agent can immediately access the exposed tool endpoints.



\paragraph{Open-source models via Ollama.}
Most open-source LLMs only understand the abstract concept of “tool calls”, typically as JSON structured actions, but they do not enforce the strict turn-based protocol and message discipline required by the \gls{mcp}.  
For this reason, the pipeline incorporates a dedicated \emph{shimming layer} implemented in \texttt{shimming\_agent.py}.  
This mechanism transparently enables the use of \gls{mcp} tools with models that lack \gls{mcp} support.

\paragraph{Shimming layer.}
The shimming agent surrounds the open-source model with a supervised loop that enforces deterministic, protocol-correct behaviour.  
It operates as follows:
\begin{enumerate}
    \item It injects a system prompt (generated from \texttt{SHIMMING\_METADATA\_SYSTEM\_PROMPT} or \texttt{SHIMMING\_VULNDECT\_SYSTEM\_PROMPT}) that strictly constrains the model:
    \begin{myitemize}
        \item the model \emph{must} output exactly one JSON object per turn;
        \item the JSON must contain either a tool call \verb|{"action": <tool>, "args": {...}}| or a final result \verb|{"action": "final", "result": {...}}|;
        \item no additional text, explanations, or code fences are permitted.
    \end{myitemize}
    \item Each assistant JSON tool call is intercepted by the shim.
    \item The call is forwarded to the appropriate \gls{mcp} backend:
    \begin{myitemize}
        \item \texttt{jadx-ai-mcp} (running on \verb|8651/sse|) for manifest, resource, and Java-side exploration;
        \item \texttt{GhidraMCP} (running on \verb|8081/sse|) for native analysis, decompilation, function/xref queries, and symbol inspection.
    \end{myitemize}
    \item The result is formatted as a new user message of the form:\\
    \verb|Tool Response: <json-encoded-result>| and sent back to the model.
    \item The loop continues until the model emits a valid \verb|"action": "final"| object that passes \texttt{pydantic} validation.
\end{enumerate}

This layer ensures that even models lacking native \gls{mcp} support behave as well-structured \gls{mcp} agents.  
Whenever the system detects malformed JSON, missing fields, or any violation of the protocol, the shim responds with a corrective prompt instructing the \gls{llm} to retry.  
Only when the generated output satisfies both the protocol and the \texttt{pydantic} schema does the shimming agent return control to the main triage loop.






%\subsubsection{Ghidra}
%For integrate Ghidra in the tool was use \texttt{jadx-ai-mcp}\footnote{https://github.com/LaurieWired/GhidraMCP}

% \subsection{Use of Pydantic}

\section{POIROT Output \& Processing}


As mentioned in Chapter~\ref{chp:preliminaries}, the triage procedure begins from the output generated by POIROT.  
During its \emph{Triage} phase\footnote{\url{https://github.com/HexHive/droidot/\#triage}}, POIROT produces a file named \texttt{folder2backtraces.txt}, shown in Listing~\ref{lst:POIROT-output}.  
This file serves as the primary input for the analysis performed in this project.  
After running POIROT on a batch of applications, the resulting directory structure for the target APK appears as illustrated in Figure~\ref{fig:poirot_output}.


\begin{figure}
    \centering
    \scalebox{0.8}{\input{tikzpicture/targetAPKS}}
    \caption[Directory structure of POIROT's output]{Directory structure of the target APKs as produced by POIROT, including extracted libraries and reproduced crash outputs.}
    \label{fig:poirot_output}
\end{figure}

For each package contained in \texttt{target\_APK}, the program explores the \texttt{fuzzing\_output} directory and identifies all entries named \texttt{<functionName-Signature>} that include the file \texttt{folder2backtraces.txt}.  
From this structure, the program builds a map in which the key is the package name (\texttt{pkgName}) and the value is a list of backtrace files.  
This mapping is then used to perform the vulnerability triage.


\begin{comment}
    
The program interacts with POIROT’s directory structure as shown earlier
in Chapter~\ref{lst:POIROT-output} 6.3 \textcolor{red}{(ensure this reference matches your final numbering)}.  
Crash artefacts include stack traces, harness entry points, and reproducer
inputs. These elements are aggregated into \texttt{CrashSummary} objects.

\end{comment}

\subsection{Extraction of Native Libraries}\label{chp:LibFiltering}

POIROT already extracts the native libraries as part of its output.  
As shown in Figure~\ref{fig:poirot_output}, the \texttt{lib/} directory contains the compiled native binaries, organised by \glspl{abi} (e.g.\ \texttt{arm64-v8a}, \texttt{x86\_64}).  
The triage pipeline selects which variant to analyse using a predefined ABI preference list; in most cases, \texttt{arm64-v8a} is prioritised.

For each extracted library, the program enumerates the available symbols using the \texttt{nm} utility, which ``displays information about symbols in the specified file, which can be an object file, an executable file, or an object-file library''~\cite{ibm-nm}.  
The resulting symbol table is cross-referenced with the crash stack trace in order to identify only those libraries that appear in the execution path.  
This filtering step avoids unnecessary \gls{mcp} queries and reduces the amount of contextual information that must be supplied to the \gls{llm}.  
Moreover, Ghidra cannot reliably open a large number of libraries within the same project, making it essential to limit the analysis to the subset of binaries that are actually relevant to the crash.

The resulting filtered map is then recorded in the \texttt{CrashSummary} object associated with the crash.  
This enriched crash descriptor is subsequently passed to the triage agent, ensuring that the analysis stage focuses exclusively on the libraries that may have contributed to the fault.

\subsection{\glsxtrlong{jcg}}

A second input provided to the model through the \texttt{CrashSummary} object is the \gls{jcg}.  
The purpose of this artefact is to reconstruct the sequence of Java method invocations that lead to the JNI entry point responsible for calling the native function in which the crash occurred.  
While the native stack trace already describes the execution path within the \texttt{.so} file, the \gls{jcg} complements it with the higher-level control flow within the application.

The extraction is performed in two phases:

\begin{enumerate}
    \item \textbf{Generation of a global flow graph.}  
    A complete flow graph of the application is first generated using FlowDroid\footnote{https://github.com/secure-software-engineering/FlowDroid}, which statically analyses the APK and produces a comprehensive \gls{jcg} covering all possible execution paths.  
    The resulting structure is stored in a large JSON file. 

    \item \textbf{Efficient filtering using \texttt{rg}.}  
    Because the FlowDroid JSON may exceed multiple gigabytes, directly parsing it in memory would be inefficient.  
    Instead, the \texttt{rg} (ripgrep) utility is used to perform fast pattern searches over the file.  
    This allows the pipeline to extract only the subset of call edges relevant to the method chain leading to the JNI bridge.  
    The result is a compact, filtered \gls{jcg} that contains only the calls involved in the crash path.
\end{enumerate}

This filtered \gls{jcg} is then attached to the \texttt{CrashSummary}, enabling the \gls{llm} to reason about both native- and Java-level execution paths.  
In this way, the model can reconstruct the complete control-flow chain leading to the fault, rather than relying solely on the native stack trace generated by POIROT.

\begin{comment}
    

\section{Environment Setup}
\label{sec:container_usage}

The implementation supports a fully containerised workflow using Docker.

\subsection*{Docker Setup Script}\label{chp:docker}

Docker is used to encapsulate the entire triage system and its dependencies within a reproducible and portable container image.  
This approach guarantees that the analysis pipeline can be executed consistently across different environments, local workstations or cloud servers, regardless of the host configuration.  
The only requirement that cannot be abstracted away is the availability of a graphical environment, which is needed to run Ghidra and Jadx, as both tools expose their functionality to the \gls{mcp} server only when their GUI components are active.


The \texttt{Dockerfile} defines a minimal image that includes all components required:
\begin{myitemize}
    \item Python~3.12 runtime with all dependencies;
    \item the \texttt{pydantic} and \texttt{pydantic\_ai} libraries for typed models and agent orchestration;
    \item \texttt{Ghidra} and \texttt{Jadx} and reciprocal \gls{mcp} servers;
    \item environment configuration for \texttt{LLM\_MODEL\_NAME}, \texttt{GHIDRA\_MCP\_DIR}, and \texttt{JADX\_MCP\_DIR}.
\end{myitemize}

Two auxiliary scripts simplify the deployment and execution of the containerised system.  
The first, \texttt{dockerSetup.sh}, automates the build process and initial execution of the container. It builds the container from the provided \texttt{Dockerfile} and assigns it the name \texttt{triage\_dev}. By naming the container, its configuration and state are retained across sessions, allowing the environment to be restarted without needing to reconfig it each time.

During setup, \texttt{dockerSetup.sh} mounts two persistent volumes:
\begin{itemize}
    \item \textbf{workspace}: synchronises the project’s source code between the host machine and the container, enabling code edits and updates to take effect immediately without rebuilding the image;
    \item \textbf{APKs}: provides shared access to the APK files to be analysed. If the APKs (the POIROT’s output) are placed in this directory, the container can access them directly; otherwise, the container remains isolated from the host filesystem, ensuring safe separation of data.
\end{itemize}


\section{Command-Line Interface }

The program is executed from the command line using Python~3:

\begin{verbatim}
     python3 main.py <target_APK> [options]
\end{verbatim}

where \texttt{<target\_APK>} is the path to the POIROT output directory containing the analysed applications.  
The command-line arguments control which APKs are processed, which model and timeout are used, and where results are stored.

\textbf{Positional argument}
    \begin{myitemize}
      \item \texttt{target\_APK}: Path to the POIROT output folder (containing one subdirectory per analysed application, e.g.\ \texttt{PkgName/}).
    \end{myitemize}

\textbf{Optional arguments}
    \begin{myitemize}
      \item \texttt{--apk-list}~\textit{Path}: Path to a \texttt{.txt} file containing the list of APKs or application names to include in the triage (one per line).  
      Lines beginning with \texttt{\#} are treated as comments and ignored.  
      Only APKs present in both the directory and this list are analysed.
      
      \item \texttt{-m, --model-name}~\textit{Str}: Specifies the \gls{llm} model name to use for the triage.  
      Defaults to the value of the environment variable \texttt{LLM\_MODEL\_NAME} or \texttt{gemini-2.5-flash} if not set.
      
      \item \texttt{-o, --out-dir}~\textit{Path}: Base directory where reports are saved.  
      If omitted, a directory named \texttt{classification\_YYYY\_MM\_DD\_HH:MM} is automatically created.

      \item \texttt{--ollama-url}~\textit{str}: Base URL for the Ollama requests (default \url{localhost:11434/v1}).  

      
      \item \texttt{--timeout}~\textit{Int}: Timeout in seconds for MCP server operations (default: 180\,s).
      
      \item \texttt{-d, --debug}: Enables detailed debug logs.  
      Prints diagnostic information, subprocess output, and environment variables used for configuration.
      
      \item \texttt{-v, --verbose}: Echoes system and user prompts exchanged with the \gls{llm}, as well as the model’s intermediate reasoning.
    \end{myitemize}

\paragraph{Example.}
As shown in Listing~\ref{lst:triageExecution}, this command analyses all APKs listed in \texttt{apk\_list.txt} located inside \texttt{./target\_APK}.  
It runs the triage pipeline using the \texttt{gemini-2.5-flash} model, stores all generated reports in \texttt{./results},  
sets a timeout of 240 seconds for MCP server operations, and enables debug-mode logging.

\begin{lstlisting}[language=Python, 
caption={Execution of the vulnerability-triage pipeline using \texttt{main.py}, which orchestrates the Jadx and Ghidra MCP servers}, 
label={lst:triageExecution}]
    python3 main.py ./target_APK \
        --apk-list ./apk_list.txt \
        -m gemini-2.5-flash \
        -o ./results \
        --timeout 240 \
        --debug
\end{lstlisting}


\end{comment}

\begin{comment}
The first execution of the container requires several additional configuration steps to enable the integration of \gls{mcp} servers and authentication for Gemini-CLI (if used):

\paragraph{Gemini CLI setup.}
Gemini authentication must be performed once, as the CLI requires an OAuth credential file. Since the Docker container lacks browser access, the login is completed externally on the host machine. 

\paragraph{Jadx MCP server.}
The Jadx MCP server is generally installed automatically when the container is first built. However, if errors occur, it can be installed manually by following the instructions.

\paragraph{Ghidra MCP server.}
The Ghidra MCP server is pre-packaged in the Docker image, but not installed, since required the use of the \gls{gui}. To activate it, Ghidra must first be launched in the container using. The setup then follows the official configuration steps described in the GhidraMCP documentation.

The second script, \texttt{dockerRun.sh}, provides a simplified interface for launching the container after it has been configured. It executes only a few lines to start the existing \texttt{triage\_dev} container, resuming the previously configured environment without rebuilding or reinitialising it.

\subsection*{Runtime Script}

The script \texttt{dockerRun.sh} launches the preconfigured container
without rebuilding it, enabling rapid iteration.
\end{comment}