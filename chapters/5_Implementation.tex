\chapter{Implementation}
\label{chp:impl}

The triage system, implemented in Python3, orchestrates an \gls{llm}-based tool over crashes discovered by the fuzzing pipeline(see chapter~\ref{chp:preliminaries} (\emph{Preliminaries})). Conceptually, the program:\\
(i) ingests POIROT output (i.e. stack traces).\\
(ii) enriches them with reverse-engineering context via the \gls{mcp} (Jadx for bytecode/manifest, Ghidra for native disassembly/decompilation).\\
(iii) produces a structured judgement comprising a vulnerability likelihood, a succinct rationale anchored to concrete evidence (frames, symbols, code hunks), and a severity estimate. 

The implementation emphasises strict data modelling, explicit tool adaptors, and prompt templates specialised for each tool, so that the resulting traces are reproducible and auditable.

\textcolor{red}{Da rivedere quando il capitolo è finito}


\section{Libraries and Dependencies}
\textcolor{red}{Da vedere}
The codebase relies on a small set of focused Python libraries and external tools:
\begin{itemize}
  \item \texttt{pydantic} (v2.12.3) for strict, typed data models (e.g., crash summaries and tool responses) and runtime validation. This allows us to require that the \gls{llm} return an instance conforming to a predefined model, so the output can be parsed deterministically (without ad-hoc prompt parsing) and serialised/deserialised as JSON with minimal glue code.
  \item \texttt{pydantic\_ai} for agent composition and the \emph{\gls{mcp}} client; specifically, the \texttt{MCPServerStdio} transport is used to connect to tool servers over stdio.
  %\item \textbf{LLM providers.} The agent is provider-agnostic and supports OpenAI/Google backends; a local wrapper (\texttt{ollamaLocal.py}) is included for offline experiments.
  %\item \textbf{External tools (via MCP).} Jadx and Ghidra are accessed through dedicated \gls{mcp} adaptors; prompts and post-processing live under \texttt{MCPs/} (see below).
\end{itemize}
All configuration (model choice, tool endpoints, timeouts) is read at start-up; failures in any tool adaptor fail fast with descriptive error messages and are surfaced in the final report.

\section{Program Structure}
Figure~\ref{fig:program-structure} summarises the modules and the directory layout used during development.

\paragraph{Entry point and orchestration.}
\texttt{main.py} parses inputs (paths to POIROT outputs, model/tool configuration), instantiates the analysis \emph{agent} via \texttt{MCPs/get\_agent.py}, and coordinates the end-to-end run: load crash artefacts $\rightarrow$ enrich with tool context $\rightarrow$ run triage $\rightarrow$ emit a structured report.

\paragraph{Data model and utilities.}
\texttt{CrashSummary.py} defines the canonical in-memory representation for a crash (stack frames, symbol/offset, JNI entry point, reproducer, harness sequence, metadata). \texttt{utils.py} centralises I/O (filesystem paths, JSON/NDJSON log handling) and small helpers (e.g., normalising symbol names, safer subprocess calls). \texttt{jadx\_helper\_functions.py} offers convenience wrappers for common Jadx queries (e.g., resolve a method signature, fetch call sites, extract a minimal decompiled hunk).

\paragraph{Agent and MCP adaptors.}
\texttt{MCPs/get\_agent.py} composes the triage agent with the required tool capabilities and the selected \gls{llm} provider. \texttt{MCPs/jadxMCP.py} and \texttt{MCPs/ghidraMCP.py} implement thin clients over \texttt{MCPServerStdio}, handling request/response schemas, error propagation, and small post-processing (e.g., trimming code hunks and attaching file/offset provenance). \texttt{MCPs/ollamaLocal.py} is an optional backend to run models locally. \texttt{MCPs/vulnAssessment.py} aggregates model/tool outputs into the final, typed judgement (likelihood, confidence, CWE/notes, severity).

\paragraph{Prompt specialisation.}
Under \texttt{MCPs/prompts/}, \texttt{ghidra\_prompts.py} and \texttt{jadx\_prompts.py} hold tool-specific instructions for evidence retrieval (symbol lookup, minimal hunk extraction, JNI boundary cues). \texttt{vulnAssesment\_prompts.py} encodes the triage schema and the evidence-anchoring style used to elicit precise, verifiable rationales from the \gls{llm}.

\paragraph{Evaluation harness.}
\texttt{evaluation.py} loads saved judgements and computes summary statistics or exports artefacts for manual review; it is deliberately decoupled from the online agent to support offline auditing and ablations.



\begin{figure}
    \centering
    \scalebox{0.8}{\input{tikzpicture/program-structure}}
    \caption{Program Structure}
    \label{fig:program-structure}
\end{figure}

\textcolor{red}{DA AGGIORNARE}

\section{Docker}

\section{\gls{llm} and \gls{mcp} integration}
The program is \gls{llm}-agnostic, so the program is not working only for one specific \gls{llm} model, but can use different, this gives you the freedom to adopt any \gls{llm} that works best for your needs, easily switch between options, and integrate specialized or customer-specific models when needed. The program supports \texttt{OpenAI}, \texttt{Gemini}, \texttt{Local Ollama models} 
\textcolor{red}{TO add introfuction}

\subsection{Agent setup}
The system is provider-agnostic and selects the backend by model name. At start-up, the agent is instantiated with \texttt{pydantic\_ai.Agent}, a target \emph{output type} (a \texttt{pydantic} model), and a list of \gls{mcp} toolsets (\texttt{MCPServerStdio}). If no model is specified, the code falls back to the \texttt{LLM\_MODEL\_NAME} environment variable (default: \texttt{gemini-2.5-flash}) and constructs a Google Gemini backend; alternatively, a local backend can be created via Ollama. The agent wiring and defaults are implemented in \texttt{MCPs/get\_agent.py} and \texttt{MCPs/ollamaLocal.py}.

\paragraph{OpenAI}
The architecture permits adding OpenAI GPT models by swapping in an OpenAI-compatible model/provider within the same \texttt{pydantic\_ai.Agent} construction. Although not enabled in the current \texttt{get\_agent.py}, the same pattern used for Gemini and Ollama applies (model selection by name, identical toolset attachment, and the same typed \texttt{output\_type}).

\paragraph{Gemini}
By default, \texttt{get\_agent.py} builds a Google Gemini backend using \texttt{GoogleModel} and \texttt{GoogleProvider}, sourcing the API key from \texttt{LLM\_API\_KEY}. The default model is \texttt{gemini-2.5-flash} unless overridden by the \texttt{model\_name} parameter or \texttt{LLM\_MODEL\_NAME}. The resulting \texttt{Agent} receives the \emph{system prompt}, the strict \texttt{output\_type} (our \texttt{pydantic} schema), and the list of \gls{mcp} toolsets, enabling tool-grounded reasoning with typed outputs.

\paragraph{Local LLM (Ollama)}
For offline or on-premise execution, \texttt{ollamaLocal.py} configures a local backend by pairing \texttt{OpenAIChatModel} with \texttt{OllamaProvider} (default base URL \texttt{http://localhost:11434/v1}) and an Ollama model name (e.g., \texttt{qwen3:8b}). The agent interface remains identical (same \texttt{system\_prompt}, \texttt{output\_type}, and \gls{mcp} toolsets). At start-up, a pre-flight check can verify that the requested model is available (e.g., via \texttt{ollama list}); if missing, the program should surface a clear error and guidance to pull the model.


\subsection{\gls{mcp} setup}
For let the \gls{llm} to do the triage was use two \glspl{mcp}. Both had required extra setup for work automatically with the rest of tool.
\subsubsection{Jadx}
For integrate Jadx in the tool was use \texttt{jadx-ai-mcp}\footnote{https://github.com/zinja-coder/jadx-ai-mcp}. The \gls{mcp}-server for works required the Jadx-GUI, so the tool, when the analisis for a APK starts, start a Jadx session using the command \verb|jadx-gui <apk>|, this open a Jadx gui interface, then the program wait a timeout to be sure that the \gls{mcp}-server goes started. To handle the session and the use of Jadx exist the file \texttt{jadx\_helper\_functions.py}, that has the task to Start the Jadx Gui with the specific application, and wait the \gls{mcp} server to be started, if it doesn't start in timeout-timing, the program will stop with an error. The .py file also help to kill the jadx session when is no more needed, or there is the need to change APK, there is also a binding with the current program session, to kill jadx when the program will close.

In Figure~\ref{lst:useJadxMCP} shows h, 

\begin{lstlisting}[language=Python, caption={method get\_jadx\_metadata}, label={lst:getMetadata}]
server = make_jadx_server()
...
async with get_agent(JADX_APP_METADATA, AppMetadata, [server], model_name=model_name) as j_agent:
    j_meta = await j_agent.run("Extract app metadata from the currently open Jadx project.")
app: AppMetadata = j_meta.output   
...
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Use of MCP}, label={lst:useJadxMCP}] 
kill_jadx()  # kill previous instance (if any)
start_jadx_gui(str(apk))
# Extract metadata via Jadx MCP

try:
    appMetadata = asyncio.run(get_jadx_metadata(model_name=args.model_name, verbose=args.verbose, debug=args.debug))      
except Exception as e:
    handle_model_errors(e)
\end{lstlisting}



\subsubsection{Ghidra}
For integrate Ghidra in the tool was use \texttt{jadx-ai-mcp}\footnote{https://github.com/LaurieWired/GhidraMCP}

\subsection{Use of Pydantic}

\section{POIROT's output and use}

\textcolor{red}{Passare direttamente la cartella target\_APK, opzionale è un file .txt che contiene pkgnames, che suppora anche i commenti con \#}

\section{Triaging of Vulnerabilities}
\subsection{APK metadata extraction}
\subsection{Extraction of .so files}
\section{JSON Output}

