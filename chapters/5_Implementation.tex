\chapter{Implementation}

% NOTE FOR REVIEWER:
% This chapter has been rewritten and expanded, preserving all TODOs,
% comments, and \textcolor{red}{...} sections as required.
% The content now aligns with the conceptual design from Chapter 5.

The implementation realises the design described in Chapter~\ref{chp:design} through a Python-based orchestration layer built around three core elements: (i) structured data models, (ii) explicit adapters for tool interaction via the \gls{mcp}, and (iii) specialised prompting templates to ensure reproducible and deterministic behaviour across crash analyses.

This chapter describes the concrete system architecture, library dependencies, internal module structure, and the integration workflow between the \gls{llm}, Jadx/Ghidra \glspl{mcp}, and POIROT’s crash artefacts.
% Where relevant, we also document operational constraints, practical issues encountered during development, and the rationale behind critical implementation decisions.

\section{Libraries and Dependencies}

%The implementation is intentionally lightweight, relying on a minimal set of Python libraries and external tools. This design choice ensures portability, reproducibility, and compatibility with container-based execution environments.
Main libraries and tools adopted in this thesis:

\begin{itemize}
    \item \textbf{pydantic (v2.12.3)}  
    Used for strict, typed data modelling. Every component in the triage
    pipeline---from crash summaries to tool responses and vulnerability
    assessments---conforms to well-defined Pydantic classes.  
    This guarantees:
    \begin{enumerate}
        \item deterministic parsing of LLM outputs,
        \item structural validation of intermediate artefacts,
        \item lossless serialisation/deserialisation into JSON.
    \end{enumerate}

    \item \textbf{pydantic\_ai}  
    Provides the agent abstraction, including the \gls{mcp}
    client and the \texttt{MCPServerStdio} transport, enabling the program
    to communicate with Jadx and Ghidra \glspl{mcp} over standard I/O streams.

    \item \textbf{Jadx} and \textbf{Ghidra}  
    The reverse-engineering backends operate as standalone \glspl{mcp}
    server. Jadx exposes bytecode, manifest, and resource-level information.
    Ghidra exposes disassembly, function listings, and decompilation output.
\end{itemize}

The system also depends on the POIROT output directory produced during
fuzzing. This directory is not modified at runtime; instead, the tool
reads crash artefacts and correlates them with symbol tables extracted
from native libraries.

\section{Program Structure}

The codebase is structured around clear modular boundaries,
as illustrated in Figure~\ref{fig:implementation_structure}.

\begin{figure}[!ht]
    \centering
    %\includegraphics[width=\textwidth]{implementation_structure.pdf}
    \scalebox{0.7}{\input{tikzpicture/program-structure}}
    \caption{High-level structure of the implementation (modules, data flow, and orchestration logic).}
    \label{fig:implementation_structure}
\end{figure}

\subsection*{Entry Point and Orchestration}

The main program (\texttt{main.py}) performs the following tasks:

\begin{enumerate}
    \item parses CLI arguments for model configuration, timeout, and output path;
    \item For each apk:
    \begin{enumerate}
    \item loads POIROT crash artefacts;
    \item instantiates the LLM triage agent through \texttt{MCPs/get\_agent.py};
    \item coordinates the full pipeline:  
    \[
    \text{load app crashes} 
    \;\rightarrow\; 
    \text{load APK / library context} 
    \;\rightarrow\; 
    \text{LLM triage} 
    \;\rightarrow\;
    \text{report}
    \]
    \item generates the structured JSON report.
    \end{enumerate}
\end{enumerate}

\subsection*{Data Models}

Three core models anchor the program’s internal state:

\begin{itemize}
    \item \textbf{CrashSummary}  
    Encapsulates the processed crash artefact. Fields include:
    process termination cause, native stack frames, JNI bridge method,
    reconstructed Java call graph, and fuzzer entry point.

    \item \textbf{AppMetadata}  
    Aggregates APK-level information extracted from Jadx, including:
    package name, versioning info, SDK attributes, and application label.

    \item \textbf{VulnResult}  
    Represents the final LLM assessment and contains:
    verdict, confidence score, CWE identifiers, severity category,
    implicated libraries, supporting evidence, recommendations, assumptions,
    and limitations.
\end{itemize}

This strict modelling ensures resilience of malformed outputs from the \gls{llm}.

\subsection*{Prompt Specialisation}

Under \texttt{MCPs/prompts/}, the implementation organises a family of prompt templates:

\begin{itemize}
    \item \texttt{AppMetadata\_prompt.py}  
    Defines the extraction rules and acceptable formats for metadata
    retrieved from Jadx.

    \item \texttt{VulnDetection\_prompt.py}
    Encodes the full triage specification: the definition of what qualifies as a vulnerability, the backward data-flow reasoning procedure, the exploitation feasibility checks, and the structured JSON schema used.

    \item \texttt{Shimming\_prompts.py}  
    Defines the system prompts that enforce the use of \glspl{mcp} when interacting with models that do not natively support the protocol. \texttt{Shimming\_prompts} contains a template with text placeholders, into which the required \gls{mcp} tools and the system instructions are dynamically injected. The template also specifies the appropriate output schema, depending on whether the target task is metadata extraction or vulnerability assessment.
\end{itemize}

%These templates operationalise the prompting strategy described in Chapter~\ref{chp:design}.

\section{\gls{llm} and \gls{mcp} Integration}

The system is \gls{llm}-agnostic, it is not tied to a single model but supports multiple options. This flexibility allows selection of the provider that best fits the task, easy switching between models. Ensuring that the same triage logic can operate with different \gls{llm} vendors or even fully local models. Currently, the program supports \texttt{OpenAI}, \texttt{Gemini} and local \texttt{Ollama} models, and can be extended to include additional vendors.

The integration between the \gls{llm} and the reverse-engineering backends is realised via the \texttt{pydantic\_ai} agent framework and the standardised \gls{mcp} interface.  
This section describes the mechanisms used to initialise the agent, connect to the \glspl{mcp}, and enforce protocol correctness.

\subsection{Agent Setup}
The agent is created through the function \texttt{get\_agent} defined in \texttt{MCPs/get\_agent.py}.  
This function instantiates a \texttt{pydantic\_ai.Agent} object that coordinates the chosen \gls{llm} and a list of MCP toolsets (\texttt{MCPServerStdio}).  
The agent receives:
\begin{itemize}
    \item \emph{A system prompt}, which defines its high-level role (e.g.\ vulnerability assessor or metadata extractor) and encodes the behavioural constraints that the model must follow. This includes the JSON-only policy, the allowed response formats, and the list of tool names that the \gls{llm} may invoke.
    
    \item \emph{An output type}, such as \texttt{AppMetadata} or \texttt{VulnResult}. This schema determines how the final answer must be structured and enables automatic validation of the model’s output.
    
    \item A list of connected \emph{\gls{mcp} servers}, each represented by a \texttt{MCPServerStdio} instance. These servers expose the reverse-engineering capabilities that the agent can access whenever the model chooses to perform a tool call.

\end{itemize}

The function automatically selects the appropriate model provider according to the prefix of the model name supplied at start-up:
\begin{itemize}
    \item names beginning with \texttt{gpt-} initialise an \texttt{OpenAIChatModel} via the \texttt{OpenAIProvider};
    \item names beginning with \texttt{gemini-} initialise a \texttt{GoogleModel} via the \texttt{GoogleProvider};
    \item any other identifier defaults to a locally hosted model served through the \texttt{OllamaProvider}.
\end{itemize}
This logic allows seamless switching between cloud-based and local models.

The agent also keeps track of the number of tool calls executed during the analysis, as well as the token usage associated with each step. This includes both the input tokens sent to the model and the output tokens generated in response, enabling detailed monitoring of the interaction.

\subsubsection{Gemini-CLI.}
If the model name is specified as \texttt{gemini-cli}, the program interacts with the Gemini Command-Line Interface\footnote{https://geminicli.com/} instead of the standard \gls{api} endpoint.
%This configuration was introduced to bypass \gls{api} usage quotas and benefit from the higher request limits available with \texttt{gemini-2.5-pro}. 

The integration is handled by the helper module \texttt{MCPs/geminiCLI.py}, which provides two key functions: \texttt{query\_gemini\_cli()} and \texttt{gemini\_response\_parser()}.
%Since the Gemini CLI does not support distinct system and user prompts or structured JSON output (as required by \texttt{pydantic\_ai.Agent}), these functions implement a custom prompting and parsing layer.
\texttt{query\_gemini\_cli()} builds a unified text prompt combining the system and user contexts, executes the \texttt{gemini} command via subprocess.
The model’s response is then passed to \texttt{gemini\_response\_parser()}, which cleans the raw \gls{cli} output and attempts to parse the result as JSON. If the output cannot be validated against the expected schema, the function retries; after four failed attempts, an error is raised.


\subsubsection{Local LLM (Ollama)}
When the model name does not match the cloud prefixes (\texttt{gpt-}/\texttt{gemini-}), the agent assumes a local configuration and instantiates an \texttt{oss\_model} object backed by the \texttt{MCPs/shimming\_agent.py}. 
%As mentions in Section~\ref{sec:shimming}, some models don't have the cabability to use \gls{mcp}, e.g. open source model, so the shimming\_agent will take the action, using the \gls{llm} provided my Ollama, and extra notions provided by the system pompt, the model is able to choose the methods needed, and required it usign \verb|("action": <tool_name>, "args": ( ... ),)|, then, using BasicMCPClient of llama\_index.tools.mcp, the wrapper python code execute the call instead the model, then the response is send to the \gls{llm} as a user prompt.
%if the model have enought informations it can reply with \verb|{"action":"final","result": {...}}| with the final reply, this will check the correctness of output\_type i.e. the code will check that the moldel had reply with the required json with all the fields

\paragraph{Shimming layer.}
As discussed in Section~\ref{sec:shimming}, some models, particularly open-source ones, do not possess native support for the \gls{mcp} protocol. In these cases, the \texttt{shimming\_agent} acts as the execution layer that enables structured tool use.  
The agent runs the \gls{llm} provided by Ollama and enriches it with the behavioural rules defined in the system prompt. With these rules in place, the model can select which tool it wants to invoke and express this decision through a JSON message of the form: \verb|{"action": "<tool_name>", "args": { ... }}|.
When such an action is emitted, the Python wrapper executes the requested call on behalf of the model. This is done via the \texttt{BasicMCPClient} from \texttt{llama\_index.tools.mcp}, which sends the arguments to the corresponding \gls{mcp} server (e.g.\ Jadx or Ghidra) and collects the response.
The result returned by the \gls{mcp} server is then forwarded back to the \gls{llm} as a new user prompt. This iterative loop continues until the model determines that it has gathered sufficient evidence to produce the final structured output. 
At that point, the model returns a JSON object of the form: \verb|{"action": "final", "result": { ... }}|.

The agent then validates this object against the expected \texttt{output\_type} (e.g.\ \texttt{AppMetadata} or \texttt{VulnResult}).  
This validation ensures that: the model has produced a well-formed JSON object, all mandatory fields are present and the types match the schema.
Only when these conditions are satisfied does the agent return the final result. Otherwise, the pipeline is interrupted, signalling that the model produced an incomplete or invalid output.

%This mechanism allows models without native \gls{mcp} support to perform multi-step, tool-augmented reasoning while ensuring that all interactions remain safe, structured, and reproducible.

%This configuration enables on-premise execution through Ollama, ensuring full data isolation and allowing the entire triage pipeline to run in offline or restricted environments.


%

\subsection{MCP Setup [MCP setup]}

\colorbox{red}{Jadx and Ghidra are launched as independent \glspl{mcp} servers. The Python orchestration layer connects to each server using \texttt{MCPServerStdio}, exposing a unified interface to the LLM.}

\begin{comment}
The shim layer enforces:
    
\begin{itemize}
    \item correct routing (Jadx for Java context, Ghidra for native code),
    \item one tool call per turn,
    \item error-handling and automatic retries,
    \item normalisation of malformed JSON responses,
    \item timeouts imposed via CLI parameters.
\end{itemize}
\end{comment}

This ensures deterministic behaviour even when using stochastic models.


\subsection{\gls{mcp} setup}

To enable the \gls{llm} to automatically perform triage, two \glspl{mcp} are used: one for \texttt{Ghidra} and one for \texttt{Jadx}.  
Both require additional setup to operate autonomously within the triage pipeline.

\subsubsection{Jadx}

The integration of Jadx into the pipeline relies on the \texttt{jadx-ai-mcp}\footnote{\url{https://github.com/zinja-coder/jadx-ai-mcp}} server, which exposes Jadx features through the \gls{mcp} protocol.  
Listing~\ref{lst:jadxServer} shows how the \gls{mcp} connection for Jadx is established using the \texttt{MCPServerStdio} interface, invoking the \texttt{jadx\_mcp\_server.py} script through \texttt{uv} from the directory specified in the environment variable \texttt{JADX\_MCP\_DIR}.


\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_jadx\_server} used to initialise the Jadx MCP connection}, label={lst:jadxServer}]
def make_jadx_server(timeout: int = 60) -> MCPServerStdio:
    return MCPServerStdio(
        "uv",
        args=["--directory", os.getenv("JADX_MCP_DIR"), "run", "jadx_mcp_server.py"],
        timeout=timeout,
    )
\end{lstlisting}


This server depends on the graphical Jadx application (\texttt{jadx-gui}) being active.  
Therefore, when analysis of an \gls{apk} begins, the program launches a dedicated Jadx session using the command: \verb|jadx-gui <apk>| 
This opens the \gls{gui} instance and allows the \texttt{jadx-ai-mcp} plugin to start its server.  

Session management is handled by the helper module \texttt{jadx\_helper\_functions.py}, which contains utilities to start, monitor, and terminate the Jadx process.  
The function \texttt{start\_jadx\_gui()} launches the GUI, monitors its output stream, and blocks execution until the MCP server is detected or a timeout occurs.  
If Jadx fails to start within the configured timeout, the function terminates the process and aborts the analysis.  
The module provides \texttt{kill\_jadx()}, used to gracefully stop the Jadx process when it is no longer required or when switching to a different \gls{apk}.  

Once the server is running, the agent connects to the \texttt{jadx-ai-mcp} endpoint to query project-level information.  
Listing~\ref{lst:getMetadata} shows how the system retrieves the \gls{apk} metadata using a dedicated agent call.

\begin{lstlisting}[language=pyMCP, caption={Method \texttt{get\_jadx\_metadata} that returns the app metadata from Jadx}, label={lst:getMetadata}]
server: MCPServerStdio = make_jadx_server()
...
async with get_agent(JADX_APP_METADATA, AppMetadata, [server], model_name=model_name) as j_agent:
    j_meta = await j_agent.run("Extract app metadata from the currently open Jadx project.")
appMetadata: AppMetadata = j_meta.output   
...
\end{lstlisting}

\paragraph{\texttt{jadx-ai-mcp} tools.}
The \gls{mcp} server exposes a set of structured tools that allow the agent to query, explore, and manipulate the currently opened Jadx project.  
The tools provaided are:
\begin{myitemize}
  \item \texttt{get\_android\_manifest}, \texttt{get\_main\_activity\_class}, \\\texttt{get\_main\_application\_classes\_names}/\texttt{\_code}: recover app entry points and context.
  \item \texttt{get\_all\_classes}, \texttt{get\_class\_source}, \texttt{get\_methods\_of\_class}, \texttt{get\_method\_by\_name}: class/method discovery and source retrieval.
  \item \texttt{search\_method\_by\_name}, \texttt{get\_smali\_of\_class}: locate JNI bridges and low-level details.
  \item \texttt{get\_all\_resource\_file\_names}, \texttt{get\_resource\_file}, \texttt{get\_strings}: resource inspection for configuration-driven behaviours.
  \item \texttt{fetch\_current\_class}, \texttt{get\_selected\_text}: GUI-assisted context capture during investigations.
  \item \texttt{rename\_class}/\texttt{rename\_method}/\texttt{rename\_field}: optional refactoring to improve readability (kept out of formal evidence).
\end{myitemize}

\subsubsection{Ghidra}
The integration of Ghidra into the pipeline relies on the \texttt{GhidraMCP}\footnote{\url{https://github.com/LaurieWired/GhidraMCP}}, which exposes Ghidra’s analysis and decompilation capabilities through the \gls{mcp} protocol. %Listing~\ref{lst:ghidraServer} shows how the \gls{mcp} connection is established using the \texttt{MCPServerStdio} interface.

\begin{comment}
    
\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_ghidra\_server} used to initialise the Ghidra MCP connection}, label={lst:ghidraServer}]
def make_ghidra_server(debug: bool = False, verbose: bool = False, timeout: int = 120) -> MCPServerStdio:
    ghidra_mcp_dir = os.getenv("GHIDRA_MCP_DIR")
    ghidra_dir = os.getenv("GHIDRA_INSTALL_DIR")
        
    return MCPServerStdio(
        "python3",
        args=[
            f"{ghidra_mcp_dir}/bridge_mcp_ghidra.py",
            "--ghidra-server", "http://127.0.0.1:8080/",
        ],
        env={"GHIDRA_INSTALL_DIR": ghidra_dir},
        timeout=timeout,
    )
\end{lstlisting}
\end{comment}

Unlike Jadx, Ghidra does not natively support project creation and binary import directly from the terminal, which complicates its automation.  
To overcome this limitation, the pipeline employs \texttt{ghidra-cli}\footnote{\url{https://github.com/Denloob/ghidra-cli}}, a command-line utility that allows project creation and binary import using simple commands such as: \verb|ghidra-cli -n -i file.so|
This command launches the Ghidra \gls{gui} (Figure~\ref{fig:ghidraGUI}) and automatically imports the specified ELF binaries (\texttt{.so} files extracted from the target \gls{apk}) into a new project.  
However, the Ghidra \gls{mcp} server becomes available only after the binary is opened in the CodeBrowser (the main Ghidra interface, represented by the dragon icon), and this selection cannot be made through the command line.

To fully automate this step, the helper module \texttt{ghidraMCP\_helper\_functions.py} was developed.  
This module uses \texttt{wmctrl} to manage desktop windows and \texttt{pyautogui} to simulate keyboard input (e.g., \texttt{Tab}, \texttt{Down Arrow}, and \texttt{Enter}) to select and open the imported file within the GUI automatically.

%The integration of Ghidra into the pipeline relies on the \texttt{GhidraMCP}\footnote{\url{https://github.com/LaurieWired/GhidraMCP}}, which exposes Ghidra’s analysis and decompilation capabilities through the \gls{mcp} protocol.  
%Unlike Jadx, Ghidra doesn't supports the start and import of new projects in an easy way from therminal, it have some issues, a work around was to use \texttt{ghidra-cli}\footnote{\url{https://github.com/Denloob/ghidra-cli}}, That allows to easily create new project and import binary using \gls{cli}, by using the command \verb|ghidra-cli -n -i ./my_binary|, this start the Ghidra \gls{gui} (Figure~\ref{fig:ghidraGUI}), importing the ELF binaries (\texttt{.so} files extracted from the target \gls{apk}) in a random name project, the problem for here is that there isn't a cli way to select the imported file, and the Ghidra \gls{mcp} starts only when the binary is launched with che Ghidra's CodeBrowser (the dragon Icon). To Automate this operation \texttt{ghidraMCP\_helper\_functions.py} was developed, this helper modul use \texttt{wmctrl} to switch pc windows and \texttt{pyautogui} to send keyboard signal, as `tab`, `down arrow` and `enter` to select the file automaticly from cli.


\begin{figure}
    \centering
    \includegraphics[width=12cm]{figures/GhidraGUI.jpeg}
    \caption{Ghidra GUI after being launched via \texttt{ghidra-cli}.}
    \label{fig:ghidraGUI}
\end{figure}

Session management is entirely handled within \texttt{ghidraMCP\_helper\_functions.py}, which provides utilities to launch, monitor, and close Ghidra processes.  
The function \texttt{openGhidraGUI()} invokes \texttt{ghidra-cli} to initialise the environment, ensures no other active Ghidra instances are running, and confirms successful startup through window detection.  
If a window titled “Ghidra:” is already open, it is automatically closed before the new session begins.  
The helper then brings the GUI to the foreground and completes the import process through simulated keyboard input.  
A built-in timeout ensures that the Ghidra environment is properly initialised before the analysis continues.


Once the \gls{gui} environment is ready, the triage agent establishes communication with the Ghidra MCP server through the \texttt{make\_ghidra\_server()} function defined in \texttt{ghidraMCP.py}.  

\begin{comment}
    
Listing~\ref{lst:ghidraServer}

\begin{lstlisting}[language=pyMCP, caption={Function \texttt{make\_ghidra\_server} used to initialise the Ghidra MCP connection}, label={lst:ghidraServer}]
def make_ghidra_server(debug: bool = False, verbose: bool = False, timeout: int = 120) -> MCPServerStdio:
    ghidra_mcp_dir = os.getenv("GHIDRA_MCP_DIR")
    ghidra_dir = os.getenv("GHIDRA_INSTALL_DIR")
        
    return MCPServerStdio(
        "python3",
        args=[
            f"{ghidra_mcp_dir}/bridge_mcp_ghidra.py",
            "--ghidra-server", "http://127.0.0.1:8080/",
        ],
        env={"GHIDRA_INSTALL_DIR": ghidra_dir},
        timeout=timeout,
    )
\end{lstlisting}

Listing~\ref{lst:mcpVulnAssessment} shows the use of \texttt{jadx-ai-mcp} and \texttt{GhidraMCP} for perform vulnerability triage process. 
\end{comment}



\paragraph{\texttt{GhidraMCP} tools.}
The Ghidra MCP server exposes a variety of analysis and decompilation tools accessible to the triage agent through structured \gls{mcp} calls.  
The main capabilities exposed through the server are:

\begin{myitemize}
  \item \textbf{Decompilation and Disassembly:}  
  \texttt{disassemble\_function} (return: assembly text).
  \texttt{decompile\_function}, \texttt{decompile\_function\_by\_address} (return: C code);  
  
  \item \textbf{Function and Address Queries:}  
  \texttt{get\_current\_address}, \texttt{get\_current\_function},  
  \texttt{get\_function\_by\_address} (return: name/object),  
  \texttt{get\_function\_xrefs}, \\ \texttt{get\_xrefs\_from}, \texttt{get\_xrefs\_to} (return: reference lists).
  
  \item \textbf{Listing and Enumeration:}  
  \texttt{list\_functions}, \texttt{list\_methods}, \texttt{list\_classes}, \\
  \texttt{list\_namespaces}, \texttt{list\_segments}, \texttt{list\_data\_items},  \texttt{list\_imports},  \\ 
  \texttt{list\_exports},  \texttt{list\_strings} (return: lists of names, symbols, or values).
  
  \item \textbf{Search and Cross-References:}  
  \texttt{search\_functions\_by\_name} (substring search),  \\
  \texttt{get\_function\_xrefs}, \texttt{get\_xrefs\_from}, \\
  \texttt{get\_xrefs\_to} (return: reference objects).
  
  \item \textbf{Editing and Annotation:}  \texttt{rename\_function\_by\_address}, \texttt{rename\_function}, \\
  \texttt{set\_function\_prototype}, \texttt{set\_local\_variable\_type},  \texttt{rename\_variable}, \\
\texttt{rename\_data},  \texttt{set\_decompiler\_comment}, \texttt{set\_disassembly\_comment} (return: flag or void).
\end{myitemize}


\subsubsection{Gemini-cli and Open Source \gls{llm}}

When the selected model is neither a cloud-based GPT nor Gemini model, the system falls back to a local execution strategy.  
Two pathways are implemented: one based on \texttt{gemini-cli} and one based on open-source models served through \texttt{Ollama}.  
Both approaches differ from the standard \texttt{MCPServerStdio}-based configuration, since neither \texttt{gemini-cli} nor most open-source models provide native support for the \gls{mcp} protocol.

\paragraph{Gemini-cli.}
The integration of Gemini models is handled through the standalone \texttt{gemini} command-line interface.  
Rather than establishing a persistent \gls{mcp} transport via \texttt{MCPServerStdio}, the system invokes \texttt{gemini-cli} as a subprocess and streams its JSON-formatted output.

A key distinction is that \texttt{gemini-cli} is capable of launching its own MCP servers automatically, based on the configuration specified in the user's \texttt{.gemini/settings.json} file.  
This configuration defines the \gls{mcp} backends to load—typically a \texttt{jadx-mcp} server and a \texttt{ghidra-mcp} server, by providing the command and arguments for each.  
When \texttt{gemini-cli} starts, it spawns the corresponding \gls{mcp} processes and exposes them to the model as tool endpoints.  

\paragraph{Open-source models via Ollama.}
Most open-source LLMs only understand the abstract concept of “tool calls”, typically as JSON structured actions, but they do not enforce the strict turn-based protocol and message discipline required by the \gls{mcp}.  
For this reason, the pipeline incorporates a dedicated \emph{shimming layer} implemented in \texttt{shimming\_agent.py}.  
This mechanism transparently enables the use of MCP tools with models that lack MCP support.

\paragraph{Shimming layer.}
The shimming agent surrounds the open-source model with a supervised loop that enforces deterministic, protocol-correct behaviour.  
It operates as follows:
\begin{enumerate}
    \item It injects a system prompt (generated from \texttt{SHIMMING\_METADATA\_SYSTEM\_PROMPT} or \texttt{SHIMMING\_VULNDECT\_SYSTEM\_PROMPT}) that strictly constrains the model:
    \begin{myitemize}
        \item the model \emph{must} output exactly one JSON object per turn;
        \item the JSON must contain either a tool call \verb|{"action": <tool>, "args": {...}}| or a final result \verb|{"action": "final", "result": {...}}|;
        \item no additional text, explanations, or code fences are permitted.
    \end{myitemize}
    \item Each assistant JSON tool call is intercepted by the shim.
    \item The call is forwarded to the appropriate MCP backend:
    \begin{myitemize}
        \item \texttt{jadx-ai-mcp} (running on \verb|8651/sse|) for manifest, resource, and Java-side exploration;
        \item \texttt{GhidraMCP} (running on \verb|8081/sse|) for native analysis, decompilation, function/xref queries, and symbol inspection.
    \end{myitemize}
    \item The result is formatted as a new user message of the form:\\
    \verb|Tool Response: <json-encoded-result>| and sent back to the model.
    \item The loop continues until the model emits a valid \verb|"action": "final"| object that passes \texttt{pydantic} validation.
\end{enumerate}

This layer ensures that even models lacking native \gls{mcp} support behave as well-structured \gls{mcp} agents.  
Whenever the system detects malformed JSON, missing fields, or any violation of the protocol, the shim responds with a corrective prompt instructing the \gls{llm} to retry.  
Only when the generated output satisfies both the protocol and the \texttt{pydantic} schema does the shimming agent return control to the main triage loop.






%\subsubsection{Ghidra}
%For integrate Ghidra in the tool was use \texttt{jadx-ai-mcp}\footnote{https://github.com/LaurieWired/GhidraMCP}

% \subsection{Use of Pydantic}

\section{POIROT Output Processing [POIROT’s output and use]}

The program interacts with POIROT’s directory structure as shown earlier
in Figure~6.3 \textcolor{red}{(ensure this reference matches your final numbering)}.  
Crash artefacts include stack traces, harness entry points, and reproducer
inputs. These elements are aggregated into \texttt{CrashSummary} objects.

\subsection{Extraction of Native Libraries [Extraction of .so files]}

Native libraries are enumerated by inspecting the \texttt{lib/} directory
within each APK. The implementation follows ABI-preference rules
(e.g.\ preferring \texttt{arm64-v8a}) and extracts relevant symbols via the
Unix \texttt{nm} utility.

The symbol table is then correlated with the stack trace to select only the
libraries relevant to the crash. This prevents unnecessary \gls{mcp} calls
and reduces LLM context size.

\section{Containerised Execution and Environment Setup [How to use]}
\label{sec:container_usage}

The implementation supports a fully containerised workflow using Docker.
This ensures reproducibility across:
\begin{itemize}
    \item local development machines,
    \item cloud-hosted environments,
    \item continuous integration pipelines.
\end{itemize}

\subsection*{Docker Setup Script}

The script \texttt{dockerSetup.sh}:
\begin{itemize}
    \item builds the container image,
    \item installs Jadx and Ghidra \glspl{mcp},
    \item configures environment variables,
    \item mounts persistent volumes for source code and APKs.
\end{itemize}

\textcolor{red}{Verify whether the description of the Ghidra MCP installation needs
additional detail regarding GUI activation.}

\subsection*{Runtime Script}

The script \texttt{dockerRun.sh} launches the preconfigured container
without rebuilding it, enabling rapid iteration.

\section{Command-Line Interface [Program arguments]}

The tool is executed via:

\begin{verbatim}
python3 main.py <target_APK> [options]
\end{verbatim}

Supported options include:
\begin{itemize}
    \item \texttt{--apk-list}: filter APKs via a user-provided list;
    \item \texttt{-m/--model-name}: select the LLM model;
    \item \texttt{-o/--out-dir}: specify the output directory;
    \item \texttt{--timeout}: impose per-operation timeouts;
    \item \texttt{-d/--debug}: enable verbose diagnostic logging;
    \item \texttt{-v/--verbose}: echo prompts and LLM intermediate steps.
\end{itemize}

An example invocation is shown in Listing~\ref{lst:example_run}.

\begin{lstlisting}[language=Python, 
caption={Execution of the vulnerability-triage pipeline using \texttt{main.py}, which orchestrates the Jadx and Ghidra MCP servers}, 
label={lst:example_run}]
    python3 main.py ./target_APK \
        --apk-list ./apk_list.txt \
        -m gemini-2.5-flash \
        -o ./results \
        --timeout 240 \
        --debug
\end{lstlisting}