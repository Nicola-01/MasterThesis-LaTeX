\chapter{Related Work}
\label{chp:relatedWork}


% Dalla Losiouk
% When Fuzzing Meets LLMs: Challenges and Opportunities
% PromeFuzz: A Knowledge-Driven Approach to Fuzzing Harness Generation with Large Language Models

%\section{Related Work}

The thesis project addresses a timely and relevant topic: the use of \glspl{llm} for automated vulnerability classification. Here, it is applied to Android native libraries. While recent research suggests that \glspl{llm} can support parts of the vulnerability analysis workflow by leveraging advanced natural language and code understanding, empirical evidence indicates that their effectiveness is strongly dependent on the quality and completeness of the context available to the model. 


The need for scalable triage of crashes, where manual analysis is often slow, labour-intensive, and difficult to apply consistently at scale, has motivated the exploration of \glspl{llm} as a complementary solution. These models can assist, or partially replace, manual inspection by rapidly interpreting crash reports, extracting relevant execution details, and supporting the identification of potential vulnerability patterns.


% This area has gained attention due to the need for scalable triage of crashes, where manual analysis is often slow, labour-intensive, and difficult to apply consistently at scale. In this context, the use of \glspl{llm} has emerged as a promising direction, as these models can either complement or partially replace manual inspection by rapidly interpreting crash reports, extracting relevant execution details, and assisting in the identification of potential vulnerability patterns.


% ----

Several studies have shown that \glspl{llm} often struggle when analysing security-critical code, especially when key semantic or structural cues are missing. For example, Basic and Giaretta~\cite{basic2024large} report that current models frequently misclassify vulnerabilities when the prompt lacks essential information about the execution environment or the underlying fault mechanism. These limitations suggest that \glspl{llm} rely on superficial patterns when deeper program semantics are inaccessible. In this context, enabling the model to autonomously explore the codebase through the \gls{mcp} framework is a promising direction: allowing the \gls{llm} to retrieve files, inspect call paths, and gather execution context can mitigate the shortcomings of prompt-only approaches and support more reliable crash interpretation.

%Recent work has also aimed to improve the reliability of \gls{llm}-assisted vulnerability analysis. Amusuo et al.\ \cite{amusuo2025falsecrashreducermitigatingfalsepositive} show that many crashes generated by OSS-Fuzz-Gen are not reachable from real entry points, and introduce an agent-based validation method that analyses stack traces and call graphs to filter false positives, underscoring the need for contextual grounding.
%Jiang et al.\ \cite{jiang2024fuzzing} further identify recurring issues in \gls{llm}-assisted fuzzing—such as limited long-context understanding and inaccurate modelling of bug-detection mechanisms—highlighting the importance of hybrid designs that combine program analysis with \gls{llm} reasoning.

%Knowledge-driven and retrieval-augmented approaches have proven effective in addressing these issues. PromeFuzz \cite{liupromefuzz} integrates API documentation, call correlations, and code metadata to guide harness generation and crash triage, with a sanitizer module that inspects call stacks and surrounding code to improve precision. Fuzz4All \cite{10.1145/3597503.3639121} focuses on automated input generation, showing how well-designed prompts can significantly influence downstream performance.

%Beyond fuzzing, s
Several works explore \glspl{llm} for vulnerability triage. CASEY \cite{torkamani2025streamlining} uses contextual information from the \gls{nvd} to automate \gls{cwe} and severity classification, while Akuthota et al.\ \cite{10456393} demonstrate similar trends in vulnerability detection and monitoring. A broader evaluation by Yin et al.\ \cite{10706805} shows that although \glspl{llm} underperform in pure detection tasks, their accuracy improves substantially when supplied with additional contextual information such as file names, commit messages, or \gls{cve} descriptions. These results further justify the use of \gls{mcp} to supply structured context directly from the codebase.

Finally, work on automated vulnerability repair reinforces the need for external feedback. VRpilot \cite{10.1145/3664646.3664770} shows that reasoning steps and validation signals significantly improve repair quality, while LProtector \cite{sheng2024lprotectorllmdrivenvulnerabilitydetection} combines GPT-4o with \gls{rag} to enhance binary vulnerability detection. Both studies illustrate that \glspl{llm} perform best when supported by external signals and structured knowledge.

Overall, the literature indicates that \glspl{llm} alone are insufficient for accurate vulnerability classification or crash triage. Their performance improves markedly when complemented by structured context, execution information, and auxiliary analysis tools. These findings motivate the integration of \gls{mcp}, enabling the \gls{llm} to ground its decisions in concrete code evidence rather than prompt-only heuristics.


\begin{comment}
    

Several studies have shown that \glspl{llm} often struggle when analysing security-critical code, especially when key semantic or structural cues are missing. For example, Basic and Giaretta~\cite{basic2024large} report that current models frequently misclassify vulnerabilities when the prompt lacks essential information about the execution environment or the underlying fault mechanism. These limitations point to a broader pattern: \glspl{llm} tend to rely on surface-level signals when deeper program semantics are inaccessible. In this context, enabling the model to autonomously explore the codebase through the \gls{mcp} framework represents a promising direction. Allowing the \gls{llm} to retrieve relevant files, inspect call paths, and gather execution context directly from the environment can mitigate shortcomings observed in prompt-only approaches and provide a more reliable basis for crash interpretation and vulnerability classification.

A significant portion of recent work has focused on improving the reliability of \gls{llm}-assisted vulnerability analysis. Amusuo et al.\ \cite{amusuo2025falsecrashreducermitigatingfalsepositive} address the high false-positive rate in OSS-Fuzz-Gen, showing that many \gls{llm}-generated crashes are not actually reachable from real entry points. Their agent-based framework analyses stack traces, call graphs, and feasibility conditions to filter out spurious crashes, emphasising the need for contextual grounding in any automated triage system.

More broadly, Jiang et al.\ \cite{jiang2024fuzzing} identify five recurring challenges in \gls{llm}-assisted fuzzing, including limited long-context understanding and inaccurate modelling of bug-detection mechanisms. These issues reduce the effectiveness of applying \glspl{llm} directly to program semantics and motivate hybrid approaches that combine traditional program analysis with \gls{llm}-based reasoning.

Knowledge-driven and retrieval-augmented strategies have shown particular promise. PromeFuzz \cite{liupromefuzz} integrates API documentation, call correlations, and code metadata into a structured knowledge base to support fuzzing harness generation and crash triage. Its sanitizer module analyses repeated crashes using call stacks and surrounding code, demonstrating that structured context significantly enhances the precision of \gls{llm}-assisted vulnerability identification.

Other work explores input generation and automated testing. Fuzz4All \cite{10.1145/3597503.3639121} introduces an autoprompting mechanism that distils program behaviour into effective prompts for generating diverse and valid inputs. Although primarily focused on fuzzing, this work highlights how the design and quality of prompts directly influence downstream analysis.

Beyond fuzzing, several studies examine the use of \glspl{llm} for vulnerability triage. CASEY \cite{torkamani2025streamlining} employs prompt engineering and contextual information from the NVD to automate CWE classification and severity assessment, demonstrating moderate accuracy for practical triage tasks. Akuthota et al.\ \cite{10456393} similarly evaluate \glspl{llm} for vulnerability detection and monitoring, reporting that performance depends heavily on structured input and multi-step analysis workflows.

A broader evaluation of open-source models by Yin et al.\ \cite{10706805} shows that while \glspl{llm} generally underperform transformer-based baselines in pure vulnerability detection, they improve substantially when provided with additional contextual information such as file names, commit metadata, and CVE descriptions. Their findings further support the need for context-rich pipelines, motivating the choice of integrating \gls{mcp} to provide the model with access to code, symbols, and call paths.

Finally, research on automated vulnerability repair reinforces the value of structured feedback. VRpilot \cite{10.1145/3664646.3664770} shows that reasoning steps and patch-validation feedback significantly enhance \gls{llm} repair accuracy. Similarly, LProtector \cite{sheng2024lprotectorllmdrivenvulnerabilitydetection} combines GPT-4o with retrieval-augmented generation to improve binary vulnerability detection on Big-Vul. Both results illustrate how \glspl{llm} benefit from external signals, validation layers, and knowledge retrieval—principles directly aligned with the design choices adopted in this project.

Overall, existing literature indicates that \glspl{llm} alone are insufficient for accurate vulnerability classification or crash triage. Their performance improves markedly when supported by structured knowledge, execution context, and auxiliary analysis tools. These insights strongly motivate the integration of \gls{mcp} within our triage pipeline, enabling the \gls{llm} to ground its decisions in concrete code evidence rather than prompt-only heuristics.


\end{comment}
\begin{comment}
    

The present project addresses a timely and increasingly relevant topic: the use of \glspl{llm} for automated vulnerability classification, in this thesis, adopted in Android native libraries.

"Recent advancements have shown that \glspl{llm} offer a promising solution by automating key
aspects of this process through advanced natural language understanding, have shown promising potential in vulnerability detection," ( da sistemare è copia incolla

This area has gained attention due to the growing complexity of software ecosystems and the practical need to perform large-scale triage of crashes and memory errors. 

Recent studies have shown that \glspl{llm} often struggle when analysing security-critical code, especially when the available context is incomplete. For example, Basic and Giaretta~\cite{basic2024large} report that current models tend to misclassify potential vulnerabilities when key structural or semantic cues are missing, indicating a strong dependence on the amount and quality of information provided in the prompt.

In this context, enabling the model to autonomously explore the codebase through the \gls{mcp} represents a promising direction. Allowing the \gls{llm} to retrieve relevant files, inspect call paths, and gather the necessary execution context may reduce some of the limitations observed when the analysis relies exclusively on manually crafted prompts. 

%Jiang et al.~\cite{jiang2024fuzzing} provide a comprehensive analysis of challenges observed in \gls{llm}-assisted fuzzing, identifying recurrent issues such as error-prone driver generation, limited generalisation to unseen code, insufficient input diversity, and unreliable bug-detection reasoning. Their work emphasises that \glspl{llm}, while powerful, require well-designed supporting mechanisms to ensure accurate and stable results.

%Liu et al.~\cite{liupromefuzz} propose \emph{PromeFuzz}, a knowledge-driven framework that integrates code-metadata extraction, documentation-based constraint retrieval, and API-correlation analysis to guide harness generation. Their results show that combining \gls{llm}-based generation with structured knowledge and systematic sanitisation significantly improves branch coverage and reduces false positives.

%Overall, the literature suggests that \gls{llm}s can meaningfully contribute to vulnerability analysis only when complemented by automated context retrieval, constraint enforcement, and iterative validation—principles that also motivate the design choices of this project.

Recent research has highlighted both the potential and the limitations of using \gls{llm}s for software vulnerability analysis, especially in contexts where models must reason about low-level behaviour such as memory safety issues in native Android libraries. Several studies point out that \gls{llm}s perform inconsistently when the available context is partial, ambiguous, or requires semantic reconstruction of execution flows. This observation aligns with the motivation behind our approach, which leverages the \gls{mcp} framework to give the \gls{llm} direct access to code, symbols, and stack traces.

A key challenge identified in the literature concerns the high rate of false positives in \gls{llm}-generated fuzzing or triage pipelines. Amusuo et al.\ \cite{amusuo2025falsecrashreducermitigatingfalsepositive} show that OSS-Fuzz-Gen produces a substantial number of crashes that are not truly reachable, and propose agent-based validation to analyse stack traces, call graphs, and feasibility conditions. Their findings underline that accurate crash assessment requires contextual information beyond the crash signature itself.

More broadly, Jiang et al.\ \cite{jiang2024fuzzing} identify five recurring challenges in \gls{llm}-assisted fuzzing, including limited long-context understanding and inaccurate modelling of bug-detection mechanisms. These limitations reduce the effectiveness of direct \gls{llm}-based reasoning about program semantics, and motivate hybrid approaches that combine static or dynamic analysis with \gls{llm}-generated insights.

Knowledge-driven and retrieval-augmented strategies have recently been shown to improve reliability. PromeFuzz \cite{liupromefuzz} integrates API documentation, call correlations, and structured metadata into a dedicated knowledge base, enabling more accurate harness generation and crash triage. Its sanitizer module analyses repeated crashes by inspecting call stacks, surrounding code, and API constraints, reflecting the importance of structured contextual reasoning—an idea closely related to providing \gls{llm}s with code-level access through \gls{mcp}.

Other works focus on leveraging \gls{llm}s for input generation and automated testing. Fuzz4All \cite{10.1145/3597503.3639121} introduces an autoprompting mechanism to distil target behaviour into effective prompts for generating diverse and valid fuzzing inputs. Although mainly oriented toward input synthesis, this demonstrates that prompt quality and available context critically influence downstream performance.

Several studies explore the use of \gls{llm}s directly for vulnerability triage. Torkamani et al.\ \cite{torkamani2025streamlining} present CASEY, which classifies CWEs and assesses severity using prompt engineering and contextual data from the NVD. Akuthota et al.\ \cite{10456393} evaluate \gls{llm}-based vulnerability detection and monitoring, showing moderate accuracy but emphasising the importance of structured input and multi-step analysis.

A broader multitask evaluation by Yin et al.\ \cite{10706805} shows that while \gls{llm}s struggle with pure vulnerability detection, they perform better when given richer contextual information (e.g., file names, commit messages, CVE descriptions). Their findings strongly support the need for context-rich pipelines and motivate tool-assisted exploration such as that enabled by \gls{mcp}.

Finally, related research on automated vulnerability repair, such as VRpilot \cite{10.1145/3664646.3664770}, highlights the role of reasoning steps and validation feedback in improving \gls{llm} performance. Similarly, LProtector \cite{sheng2024lprotectorllmdrivenvulnerabilitydetection} combines GPT-4o with retrieval-augmented generation for vulnerability detection and shows improved F1 scores on Big-Vul. Both approaches reinforce the idea that \gls{llm}s benefit significantly from structured external signals—either through validation, retrieval, or interactive feedback.

Overall, the literature indicates that \gls{llm}s alone are not sufficient for accurate vulnerability classification or crash triage. Their performance improves substantially when they are provided with structured knowledge, execution context, and external analysis tools. This motivates our design choice to integrate \gls{mcp} into the triage pipeline, enabling the \gls{llm} to autonomously explore the library, inspect relevant functions, and ground its reasoning in concrete code evidence.

\end{comment}
\begin{comment}
    
\section{Fuzzing of Android native libraries and harness generation}
Fuzzing of Android native libraries poses challenges beyond conventional user-space fuzzing due to the \gls{jni} boundary, lifecycle constraints, and the need for realistic cross-language call sequences. POIROT automatically synthesises consumer-specific harnesses for closed-source Android libraries by analysing Java-side usage and supporting bidirectional \gls{jni} interactions, enabling large-scale campaigns that uncovered thousands of unique crashes \cite{poirot-usenix25}. Atlas similarly targets Android closed-source native libraries with a cross-language fuzzing framework and automatic harness generation \cite{atlas-issta24}. Beyond the Android setting, FuzzGen synthesises library-specific fuzzers by inferring API contracts and integrating with \gls{libfuzzer} to reach deep states \cite{fuzzgen-usenix20}. Prior engineering work shows that reproducing \gls{art} behaviour and \gls{jni} semantics is critical for validity and reproducibility of results \cite{polito-android-native-fuzzing}. As to fuzzing engines, \gls{aflpp} extends greybox fuzzing with a rich ecosystem of mutators and instrumentation \cite{aflpp-woot20}, while \gls{libfuzzer} offers tight integration with LLVM sanitizers for in-process fuzzing \cite{libfuzzer-llvm}. Sanitizers such as \gls{asan} and \gls{hwasan} are widely used to diagnose memory-safety defects during fuzzing on Android \cite{asan-android-aosp,hwasan-ndk}.

\section{Crash triage and LLM-based vulnerability classification}
Traditional crash triage combines bucketing, deduplication, and heuristic \emph{exploitability} estimation, but typically requires expert oversight and offers limited precision across diverse crash types \cite{scb-ase18,igor-ccs21}. Recent studies investigate \gls{llm}-assisted triage. CASEY reports non-trivial accuracy for CWE classification (68\%) and severity identification (73.6\%) on an NVD-derived corpus, indicating that \gls{llm}s can streamline parts of vulnerability triage \cite{casey-aic25}. LProtector explores \gls{llm}-driven vulnerability detection for C/C++ projects with retrieval augmentation, highlighting benefits and pitfalls of model-in-the-loop security analysis \cite{lprotector-2024}. Broader surveys benchmark \gls{llm}s and agents for practical software security, consolidating evidence that model judgements improve when provided with structured context and repository-level signals, but also documenting variability across tasks and datasets \cite{acl25-llm-benchmark-slr,slr-llm-vuln-2025}. These findings motivate tool-grounded designs for triage under Android/\gls{jni} constraints.

\section{Tool grounding with \gls{mcp} for program analysis and reverse engineering}
Tool grounding aims to reduce hallucinations and improve faithfulness by letting the \gls{llm} query authoritative artefacts. The \gls{mcp} standardises how models connect to external tools and data sources \cite{mcp-overview,anthropic-mcp}. In our setting, grounding uses Jadx to retrieve bytecode/manifest context and Ghidra for disassembly/decompilation. An emerging ecosystem of \gls{mcp} servers exposes reverse-engineering capabilities of Ghidra to \gls{llm}-based clients (e.g., symbol and function listings, decompilation), facilitating evidence-linked reasoning \cite{ghidra-mcp-laurie,ghidra-mcp-suid}. While promising, the security surface of tool integrations must be considered; recent supply-chain incidents around \gls{mcp} servers highlight the need for permission scoping and integrity checks \cite{mcp-supplychain-incident}. Our design leverages \gls{mcp} to fetch verifiable snippets (stack frames, function names, minimal decompiled code) that the triager references in its rationale.

\section{Positioning}
Compared to POIROT and Atlas, which focus on automating harness generation and fuzzing at scale \cite{poirot-usenix25,atlas-issta24}, our work targets the \emph{post-fuzzing} triage stage for Android native crashes. Relative to CASEY/LProtector and survey results on \gls{llm}-for-security \cite{casey-aic25,lprotector-2024,acl25-llm-benchmark-slr,slr-llm-vuln-2025}, our contribution is a tool-grounded pipeline that couples \gls{mcp}-mediated access to Jadx/Ghidra with crash artefacts from POIROT, aiming to improve precision, explainability, and analyst verification for \gls{jni}-mediated memory-safety issues.


The proposed approach lies at the confluence of coverage-guided fuzzing and bottom-up harness generation, and recent applications of \gls{llm}s to vulnerability classification and triage. We organise prior work into four strands: (i) methodologies for automated vulnerability discovery via fuzzing; (ii) \gls{llm}-assisted harness generation and mitigation of false positives; (iii) \gls{llm}-based vulnerability triage and assessment; and (iv) reasoning and validation-feedback loops that improve reliability of automated analyses.

\section{Automated vulnerability detection and fuzzing}

Fuzz testing remains a principal technique for uncovering defects and security issues at scale. A useful distinction is between top-down fuzzing of public entry points and bottom-up fuzzing of internal functions, the latter demanding specialised drivers to translate unstructured inputs into well-formed, semantically valid API arguments \cite{jiang2024fuzzing}. Recent \gls{llm}-assisted fuzzing frameworks demonstrate that language models can increase input diversity and sustain long-horizon exploration, provided that prompting strategies adaptively distil documentation and examples into effective prompts and evolve them during the fuzzing loop \cite{10.1145/3597503.3639121}. However, bottom-up strategies are especially prone to invalid drivers and unrealistic executions, which in turn inflate the rate of spurious crashes.

\section{\gls{llm}-assisted harness generation and false-positive mitigation}

Two complementary directions address the fidelity of generated drivers and the veracity of reported crashes. First, \emph{knowledge-driven} harness generation uses code metadata, API documentation and usage correlations to guide \gls{llm}s; a \emph{sanitizer} module then fixes compilation/runtime issues and performs crash analysis to separate harness misuse from genuine bugs, feeding newly inferred constraints back into the knowledge base \cite{liupromefuzz}. Second, multi-agent schemes for large-scale driver generation propose (i) a Function Analyzer that extracts input/state constraints from code to proactively reduce misuse, and (ii) a Crash Validation agent that checks whether a crash is feasible from real entry points, filtering out a substantial fraction of false positives \cite{amusuo2025falsecrashreducermitigatingfalsepositive}. These works collectively indicate that constraint-aware generation and post hoc feasibility checks are both necessary to keep precision high when scaling bottom-up fuzzing.

\section{\gls{llm}-based vulnerability triage and assessment}

Beyond discovering crashes, several studies target automated triage: mapping issues to \gls{cwe} categories and estimating severity under \gls{cvss}. CASEY shows that \gls{llm}s can classify \gls{cwe} and approximate \gls{cvss} labels from structured descriptions and compact code hunks, with fine-tuning improving accuracy and stability on an NVD-derived dataset \cite{torkamani2025streamlining}. LProtector employs binary classification with Retrieval-Augmented Generation (RAG) to inject domain knowledge and reports improved F1 on Big-Vul, suggesting that retrieval helps compensate for limited in-context capacity \cite{sheng2024lprotectorllmdrivenvulnerabilitydetection}. A broader multi-task evaluation finds that while traditional transformers remain competitive for pure detection, code-centric \gls{llm}s excel at assessment/localisation when provided with key contextual artefacts (e.g., \gls{cve} descriptions, commits) \cite{10706805}. Earlier work on continuous monitoring likewise reports useful initial classification performance but highlights sensitivity to noisy descriptions and the need for stronger validation signals \cite{10456393}. Together, these results motivate a triage pipeline that privileges concise, structured context and emphasises evidence that analysts can verify.

\section{Reasoning and validation-feedback loops}

Reliability improves when \gls{llm} reasoning is coupled with external validation. A case study on automated vulnerability repair (VRpilot) demonstrates that explicit chain-of-thought prompts and iterative feedback from compiler errors, tests, and sanitizer diagnostics produce more plausible and correct patches than prompting alone \cite{10.1145/3664646.3664770}. By analogy, triage decisions can benefit from similar feedback: stack traces, minimized inputs, and runtime diagnostics can drive iterative re-analysis to confirm root causes and filter out artefacts of invalid harnesses or unreachable paths.

\section{Positioning with respect to Android native libraries}

The thesis specifically targets crashes uncovered when fuzzing C/C++ libraries integrated via the \gls{jni}. Prior work on \gls{llm}-assisted harnessing highlights the twin challenges that are particularly acute in the Android context: (i) constructing realistic call sequences across the managed–native boundary, and (ii) ensuring that reported crashes correspond to feasible executions rather than artefacts of driver misuse \cite{liupromefuzz, amusuo2025falsecrashreducermitigatingfalsepositive}. We therefore draw on constraint-aware driver generation and feasibility checks to motivate a triage stage that leverages structured artefacts (stack frames, call sequences, compact code snippets) and reports verifiable evidence alongside \gls{cwe}/\gls{cvss} judgements \cite{10706805, torkamani2025streamlining}.

\paragraph{Scope within this thesis.}
Our focus is on post-fuzzing \emph{triage} rather than on harness generation per se. Nevertheless, we incorporate the practical insights of knowledge-driven driver synthesis and agentic crash validation to (i) anticipate common false-positive modes during analysis and (ii) shape the evidence presented to the triager (e.g., concise hunks rather than full files), consistent with findings that excessive or noisy context degrades classification quality \cite{liupromefuzz, torkamani2025streamlining}.

\section*{\textcolor{red}{Note on tool-grounding and \gls{mcp} in cybersecurity}}
\textcolor{red}{While our evaluation relies solely on the cited literature above, our method employs tool-grounding to reduce hallucinations and improve verifiability. In particular, \gls{mcp}-mediated access to reverse-engineering tools (e.g., bytecode and native artefacts) can supply the compact, high-value context recommended by prior work. A systematic review of \gls{mcp} applications in cybersecurity will be included in a future revision, with explicit citations once curated.}
\end{comment}


\begin{comment}
    \paragraph{Summary.}
Prior work shows that \gls{llm}s can help generate harnesses, reduce false positives via constraint extraction and feasibility checking, and automate \gls{cwe}/\gls{cvss} labelling when provided with focused, structured context \cite{amusuo2025falsecrashreducermitigatingfalsepositive,liupromefuzz,torkamani2025streamlining,10706805,10.1145/3597503.3639121}. Our thesis builds on these insights to design a tool-grounded triage pipeline tailored to Android’s managed–native boundary, aiming to deliver evidence-linked, analyst-verifiable decisions for crashes produced by fuzzing native libraries.

\section{aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa}
\textcolor{red}{https://arxiv.org/pdf/2510.18508}
\end{comment}


