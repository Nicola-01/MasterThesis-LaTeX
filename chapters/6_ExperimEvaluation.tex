\chapter{Evaluation}
\label{chp:eval}

This chapter evaluates the effectiveness, reliability, and overall behaviour of the proposed triage system.  
The goal is to assess how well the \gls{llm}-based approach classifies crashes and how the availability of additional program context, i.e. \gls{jcg}, influences its decisions.  
The chapter presents the experimental setup, the selected test applications, and a quantitative evaluation based on standard classification metrics.  
The results provide insight into the strengths and limitations of the current implementation and form the basis for the discussion in the following chapter.


\section{Experimental Setup}

%, using the Docker-based environment described in Chapter~\ref{chp:docker}.

The evaluation of the tool was carried out on a local machine.  
The experimental environment consisted of:

\begin{myitemize}
    \item Local machine running \textbf{Ubuntu 25.04};
    %\item Docker container based on \textbf{Ubuntu 24.04};
    \item \textbf{Ghidra~11.4.2} (latest version at the time of evaluation);
    \item \textbf{Jadx~1.5.3} (latest version at the time of evaluation);
    \item \textbf{GhidraMCP~1.4} (latest version at the time of evaluation);
    \item \textbf{jadx-ai-mcp~4.0.0} (latest version at the time of evaluation).
\end{myitemize}

\subsection{AWS for POIROT}

The extraction of crashes used for this evaluation is described in Chapter~\ref{chp:preliminaries}, and the corresponding execution environment is detailed in Chapter~\ref{chp:preliminaries_env}.  
All POIROT runs were executed on an AWS EC2 instance configured for large-scale fuzzing.

\subsection{\glsxtrshort{llm} Used}

The evaluation was conducted using the OpenAI model GPT-5.1.  
This model represented one of the most capable \glspl{llm} available at the time of testing and offered reliable support for \gls{mcp}-based tool use.
GPT-5.1 also integrates smoothly with the Pydantic framework, allowing strict control over structured outputs.

% No fine-tuning or additional training was performed, as the goal of the evaluation was to assess how a general-purpose model behaves when equipped with structured context and targeted guidance through MCP-based evidence retrieval.


\section{Test Applications}

A total of 137 method were used for the evaluation, originating from 80 distinct applications.  
Among these 137 cases, 62 crashes involved native methods that were reachable from the Java layer and could therefore be analysed using the filtered \gls{jcg}.  \\
The remaining 75 crashes involved methods that were either not reachable from Java or for which FlowDroid failed to generate a valid call graph.  
These cases were still processed by the triage pipeline, but without Java-side context.

Table~\ref{tab:apps} summarises all applications used in the evaluation and reports, for each APK, the metrics collected during the triage process.  
The columns are defined as follows:

\begin{itemize}
    \item \textbf{App} and \textbf{Version}: package name and exact APK version analysed.
    \item \textbf{Methods}: total number of native methods evaluated for that application.  
    \item \textbf{Crashes}: number of distinct crashes produced by POIROT; a single method may generate multiple crashes.
    \item \textbf{Time (MM:SS)}: total \gls{llm} analysis time for the application (excluding Ghidra startup time).
    \item \textbf{Input token} and \textbf{Output token}: total token consumption across all \gls{llm} requests for that application.
    \item \textbf{LLM request}: number of request that the model sent to the API, during the full triage stage.
    \item \textbf{LLM Tool Calls}: number of successful MCP tool invocations (Jadx or Ghidra) performed during the analysis.
\end{itemize}

On average, the analysis time per application was 2 minutes and 38 seconds.  
Normalising by method rather than by application shows that each method produced an average of 2.13 crashes, with a mean processing time of 42 seconds per method.  
This indicates that the system is able to process individual crash reports with very low latency, especially when compared to manual inspection workflows. This make the pipeline suitable for large-scale, automated vulnerability triage.

\begin{comment}
    

Table~\ref{tab:apps} lists the applications used for the evaluation, together with the exact versions of each APK.  
These apps were selected from the POIROT dataset and filtered based on the run performed. 

Table~\ref{tab:apps_time} reports the \gls{llm}-analysis time for each application.  
The total time per APK depends primarily on the \emph{number of crashes} produced by POIROT, as well as the number of relevant libraries and the complexity of the stack trace.
Note that the reported time covers only the \gls{llm} classification phase and does not include the startup time of Ghidra, which depends solely on the number and size of the native libraries being imported.


\glspl{llm}, when accessed through their cloud \glspl{api}, compute usage costs based on the number of \emph{input tokens} (the prompt sent to the model) and \emph{output tokens} (the generated response).  
Token consumption plays a central role in both performance and cost.  
In this evaluation, usage varies primarily according to the size of the prompts (which include both the system and user components), the number of crashes produced by POIROT for each application, the amount of decompiled code retrieved from Ghidra—which grows with the depth and complexity of the stack trace—and the verbosity of the final triage explanation generated by the \gls{llm}.  


\subsection{Requests and Tool Calls}

Each crash triggers a series of \emph{LLM requests}, corresponding to the number of times the agent sends a prompt to the model, and a number of \emph{LLM tool calls}, which represent successful invocations of Jadx or Ghidra through MCP during the analysis.  

Table~\ref{tab:apps_requests} reports, for each application, the number of LLM requests and MCP tool calls performed during the full triage stage.  

Table~\ref{tab:apps_tokens} reports input and output token usage for each application.
\end{comment}

\textcolor{red}{Sto ancora cercando di sistemare questa tabella, ma intanto ve la mado così}

\textcolor{red}{In generale, essendo lunga la tabella, va bene qui o meglio nel appendix?}


\begingroup
  \small
  \setlength{\tabcolsep}{3pt}
\begin{longtable}{|p{3cm}|c|c|c|c|c|c|c|c|}
\hline
\textbf{App} & \textbf{Version} & \textbf{Methods} & \textbf{Crashes} & \textbf{Time (MM:SS)} & \textbf{Input token} & \textbf{Output token} & \textbf{LLM request} & \textbf{LLM Tool Calls} \\
\hline
\endfirsthead

\hline
\textbf{App} & \textbf{Version} & \textbf{Methods} & \textbf{Crashes} & \textbf{Time (MM:SS)} & \textbf{Input token} & \textbf{Output token} & \textbf{LLM request} & \textbf{LLM Tool Calls} \\
\hline
\endhead

br.com.pedidos10 & 1.16.4 & 1 & 2 & 01:20 & 36344 & 4006 & 5 & 9 \\
br.com.sulamerica.sam.saude & 7.66.0 & 1 & 2 & 01:33 & 59737 & 4611 & 6 & 16 \\
ca.radioplayer.android & 6.3.420.1 & 1 & 10 & 08:00 & 494465 & 23484 & 54 & 73 \\
chk.kingnet.app & 2.10.0 & 1 & 2 & 01:33 & 57633 & 5128 & 9 & 10 \\
com.ahnlab.v3mobileplus & 2.5.20.10 & 4 & 7 & 06:13 & 415345 & 20700 & 34 & 51 \\
com.amazon.avod.thirdpartyclient & 3.0.343.77747 & 1 & 4 & 02:07 & 155907 & 8867 & 16 & 12 \\
com.android.chrome & 115.0.5790.166 & 2 & 2 & 00:44 & 47771 & 2293 & 8 & 4 \\
com.appgeneration.itunerfree & 9.3.13 & 1 & 2 & 01:53 & 53776 & 6516 & 6 & 14 \\
com.bitstrips.imoji & 11.79.0.9763 & 1 & 1 & 00:33 & 12157 & 1274 & 2 & 1 \\
com.btckorea.bithumb & 3.0.2 & 1 & 1 & 00:26 & 13001 & 2572 & 2 & 3 \\
com.cisco.webex.meetings & 45.3.0 & 1 & 3 & 02:09 & 92548 & 7643 & 10 & 23 \\
com.clearchannel.iheartradio.controller & 10.36.0 & 1 & 3 & 03:20 & 151439 & 7039 & 17 & 20 \\
com.cyworld.camera & 4.4.1 & 2 & 4 & 02:07 & 64943 & 5945 & 7 & 12 \\
com.didiglobal.driver & 7.5.88 & 1 & 8 & 06:48 & 215355 & 14366 & 31 & 41 \\
com.elevenst & 9.3.4 & 1 & 1 & 01:14 & 358406 & 2707 & 9 & 9 \\
com.ford.fordpass & 4.23.1 & 1 & 4 & 03:34 & 198312 & 10642 & 17 & 42 \\
com.ford.fordpasseu & 4.23.1 & 1 & 3 & 03:05 & 84360 & 6811 & 9 & 26 \\
com.google.android.apps.translate & 7.7.0.540337148.2-release & 1 & 1 & 00:43 & 21799 & 1769 & 3 & 2 \\
com.hyundaicard.cultureapp & 1.0.72 & 1 & 1 & 00:48 & 14153 & 2152 & 2 & 3 \\
com.intsig.BCRLite & 7.85.5.20251016 & 6 & 9 & 05:41 & 195757 & 18587 & 29 & 56 \\
com.jeju.genie & 2.2.13 & 1 & 1 & 00:33 & 12987 & 2244 & 2 & 3 \\
com.kakaopay.app & 2.5.4 & 1 & 1 & 00:28 & 22759 & 1863 & 3 & 2 \\
com.kartatech.karta.gps & 2.44.02 & 1 & 1 & 00:37 & 31715 & 1993 & 4 & 8 \\
com.kbankwith.smartbank & 1.5.8 & 10 & 39 & 24:36 & 1185064 & 105964 & 145 & 253 \\
com.kbstar.kbbridge & 1.2.1 & 1 & 1 & 00:35 & 13014 & 2189 & 2 & 3 \\
com.kii.safe & 12.2.0 & 3 & 5 & 02:50 & 121176 & 8035 & 16 & 19 \\
com.kt.ktauth & 02.01.37 & 3 & 7 & 05:03 & 178725 & 15975 & 27 & 37 \\
com.mapfactor.navigator & 7.3.17 & 2 & 3 & 01:02 & 59737 & 4296 & 9 & 12 \\
com.microsoft.office.outlook & 4.2330.0 & 1 & 1 & 01:34 & 19565 & 2182 & 3 & 5 \\
com.mitake.android.bk.tcb & 3.21.0105 & 1 & 2 & 01:03 & 49766 & 3710 & 8 & 10 \\
com.mttnow.droid.easyjet & 2.70.0 & 2 & 3 & 01:27 & 80463 & 5835 & 11 & 9 \\
com.pandora.android & 2509.1 & 1 & 4 & 07:00 & 70891 & 9638 & 8 & 20 \\
com.qustodio.family.parental.control.app.screentime & 182.16.0 & 1 & 2 & 01:20 & 44536 & 5596 & 6 & 11 \\
com.realmestore.app & 1.9.0 & 1 & 1 & 00:43 & 27834 & 1979 & 4 & 10 \\
com.riffsy.FBMGIFApp & 2.1.61 & 1 & 1 & 00:32 & 34696 & 2000 & 5 & 5 \\
com.rockbite.deeptown & 6.2.10 & 1 & 1 & 00:17 & 6564 & 1421 & 1 & 0 \\
com.rstgames.durak & 1.9.11 & 1 & 1 & 00:36 & 19232 & 1726 & 3 & 3 \\
com.samsungcard.shopping & 1.4.901 & 1 & 1 & 00:22 & 12380 & 1170 & 2 & 1 \\
com.shareitagain.whatslov.app & 12.5.0 & 1 & 4 & 02:04 & 161927 & 7541 & 17 & 24 \\
com.skmc.okcashbag.home\_google & 7.0.8 & 3 & 4 & 02:56 & 93879 & 10606 & 13 & 22 \\
com.skt.prod.dialer & 13.6.5 & 2 & 7 & 04:38 & 212223 & 12100 & 26 & 26 \\
com.skt.smartbill & 6.4.1 & 5 & 16 & 09:21 & 378136 & 28641 & 54 & 77 \\
com.skysoft.kkbox.android & 6.4.60 & 2 & 3 & 02:21 & 74898 & 8196 & 11 & 12 \\
com.smg.spbs & 3.42 & 1 & 1 & 00:38 & 19257 & 2338 & 3 & 6 \\
com.sony.tvsideview.phone & 6.2.0 & 1 & 1 & 00:57 & 56892 & 2387 & 4 & 10 \\
com.sopheos.videgreniersmobile & 20.11.10 & 1 & 2 & 01:00 & 48392 & 3213 & 6 & 10 \\
com.ss.android.ugc.trill & 9.1.5 & 2 & 2 & 00:58 & 52594 & 3785 & 8 & 6 \\
com.ssg.serviceapp.android.egiftcertificate & 2.6.10 & 1 & 1 & 00:31 & 18401 & 1734 & 3 & 2 \\
com.teamjin.deliveryk & 7.0.3 & 1 & 2 & 01:23 & 31816 & 4143 & 4 & 9 \\
com.telkomsel.roli & 3.1.0 & 2 & 2 & 01:58 & 107680 & 4364 & 8 & 8 \\
com.tencent.mm & 8.0.28 & 9 & 41 & 28:21 & 2752831 & 94404 & 204 & 239 \\
com.tmon & 5.8.4 & 2 & 3 & 02:19 & 85577 & 5551 & 11 & 15 \\
com.tplink.skylight & 3.1.20 & 1 & 2 & 01:09 & 59677 & 5617 & 7 & 14 \\
com.ucturbo & 1.10.3.900 & 1 & 1 & 00:31 & 22220 & 1869 & 3 & 2 \\
com.youdao.hindict & 6.6.2 & 1 & 2 & 00:47 & 38715 & 2880 & 6 & 4 \\
fr.radioplayer.android & 6.6.420.1 & 1 & 9 & 10:28 & 202292 & 21668 & 22 & 59 \\
heartratemonitor.heartrate.pulse.pulseapp & 1.2.7 & 1 & 1 & 00:34 & 25401 & 2109 & 3 & 2 \\
kr.co.busanbank.mbp & 3.0.10 & 6 & 19 & 15:20 & 520673 & 44981 & 59 & 112 \\
kr.co.kfcc.mobilebank & 1.2.6 & 1 & 1 & 00:30 & 13046 & 1827 & 2 & 2 \\
kr.co.morpheus.geps & 02.42 & 1 & 1 & 00:59 & 22248 & 1544 & 3 & 4 \\
kr.co.samsungcard.mpocket & 5.4.306 & 2 & 2 & 00:55 & 29228 & 2915 & 4 & 8 \\
kr.go.iros & 1.2.3 & 1 & 1 & 00:43 & 24988 & 2374 & 4 & 4 \\
kr.go.kcs.mobile.pubservice & 1.0.281 & 2 & 2 & 01:00 & 25362 & 3232 & 4 & 4 \\
kr.go.minwon.m & 2.5.95 & 1 & 1 & 00:30 & 14516 & 1378 & 2 & 4 \\
kr.go.nts.android & 12.8 & 2 & 3 & 01:27 & 59025 & 4887 & 8 & 13 \\
kr.go.wetax.android & 5.4.14 & 2 & 2 & 01:30 & 27318 & 4586 & 4 & 5 \\
net.ib.android.smcard & 10.1.903 & 2 & 2 & 01:04 & 51170 & 4139 & 8 & 7 \\
net.orizinal.subway & 3.7.8 & 4 & 4 & 02:33 & 434088 & 9043 & 23 & 19 \\
nh.smart.allonebank & 1.8.8 & 1 & 1 & 00:36 & 24960 & 1695 & 4 & 4 \\
nh.smart.banking & 4.1.2 & 3 & 5 & 02:58 & 108685 & 10359 & 17 & 18 \\
nh.smart.card & 6.5.0 & 1 & 1 & 00:30 & 14111 & 1783 & 2 & 3 \\
nh.smart.nhallonepay & 3.3.7 & 2 & 2 & 01:25 & 84244 & 4386 & 12 & 11 \\
nh.smart.nhcok & 2.0.70 & 1 & 1 & 00:37 & 14361 & 1676 & 2 & 4 \\
ragazzo.alphacode.com.br & 3.0.9 & 1 & 2 & 01:27 & 38671 & 3792 & 5 & 13 \\
ru.cn.tv & 7.8.16 & 1 & 1 & 00:20 & 20412 & 1492 & 3 & 4 \\
ru.mw & 4.50.0 & 1 & 1 & 00:36 & 24321 & 2164 & 4 & 7 \\
sam.myanycar.samsungFire & 6.0.8 & 1 & 1 & 00:40 & 20620 & 2394 & 3 & 5 \\
stickermaker.stickercreater.whatsappstickers.stickermakerforwhatsapp & 1.01.40.02.08 & 3 & 3 & 02:16 & 78127 & 7106 & 12 & 17 \\
tw.com.taishinbank.ccapp & 5.631 & 1 & 2 & 01:38 & 31781 & 4205 & 4 & 10 \\
tw.gov.tra.twtraffic & 2.1.2 & 1 & 2 & 01:18 & 42834 & 3530 & 5 & 13 \\

\hline
\multicolumn{2}{|l|}{\textbf{Average:}} & \textbf{02:38} & \textbf{1.76} & \textbf{3.76} & \textbf{133581.92} & \textbf{8364.95} & \textbf{13.89} & \textbf{20.44} \\
\hline
\caption[Applications evaluated by the triage system.]{Summary of applications used in the evaluation, including per-app method counts, crash counts, processing time, token usage, LLM and tool invocations.}
\label{tab:apps}\\
\end{longtable}
\endgroup

\begin{comment}

    # Totali Raccolti:
Total Methods: 141
Total Crashes: 301
Total Time (sec): 12687
Total Input Tokens: 10686554
Total Output Tokens: 669196
Total LLM Requests: 1111
Total LLM Tool Calls: 1635

----------------------------------------
Statistiche Medie (su 80 pacchetti):
Methods Avg:    1.76
Crashes Avg:    3.76
----------------------------------------
Time Avg:       02:38 (M:S)
Input Tokens:   133581.92
Output Tokens:  8364.95
LLM Requests:   13.89
LLM Tool Calls: 20.44
----------------------------------------
Crashes per Crash:  2.1348
Avg Time for Method: 00:42 (M:S)
Avg Input Tokens per Method: 35503.5017
Avg Output Tokens per Method: 2223.2425
Avg Requests per Method: 3.6910
Avg Tool per Method: 5.4319
\end{comment}



        


\section{Evaluation Metrics}
%\textcolor{red}{non se serve specificare le metriche}


To objectively assess the performance of the \gls{llm}-based triage system, standard information retrieval metrics derived from a confusion matrix are employed. Given the binary nature of the classification task, determining whether a crash is a vulnerability or not, the possible outcomes are defined as follows:

\begin{itemize}
    \item \textbf{True Positive (TP):} A vulnerability correctly identified by the model. The crash represents a security risk, and the system correctly classified it as such (\texttt{is\_vulnerable: true}).
    \item \textbf{False Positive (FP):} A benign crash incorrectly flagged as a vulnerability. This represents a ''false alarm``, where the model assigns security relevance to a non-exploitable issue.
    \item \textbf{True Negative (TN):} A benign crash correctly identified as non-vulnerable. The system correctly determined that the issue does not pose a security risk.
    \item \textbf{False Negative (FN):} A vulnerability missed by the model. The crash is dangerous, but the system incorrectly classified it as benign.
\end{itemize}

Based on these definitions, the following metrics were calculated:

\paragraph{Accuracy}
Accuracy quantifies how many of the analysed crashes were assigned the correct label.  
It provides an overall view of the system’s correctness across both vulnerable and non-vulnerable cases.

\[
\text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}
\]

\paragraph{Precision} Precision quantifies the reliability of the positive predictions. 
Indicates how often a crash flagged as vulnerable is indeed a real vulnerability.

\[
\text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
\]

\paragraph{Recall}
Recall measures how effectively the system identifies all crashes that correspond to real vulnerabilities.  
Captures the model’s ability to avoid missing true security-relevant issues.  
Indicates how many actual vulnerabilities the system successfully detects.


\[
\text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
\]

\paragraph{F1-Score}
The F1-score provides a single value that captures the trade-off between precision and recall.  
It reflects the balance between detecting vulnerabilities and avoiding false alarms.

\[
\mathrm{F1} = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
\]

\section{Results}
\label{chp:result}

The confusion matrices in Figure~\ref{fig:classification_outcomes} summarise how the system classified the 137 methods in the dataset, separating the two experimental conditions (with and without the \glsxtrlong{jcg}) and also reporting the aggregated results.

\begin{figure}[H]
    \centering
    \input{tikzpicture/classificationResult}
    \caption[Performance metrics for the vulnerability classification task]%
{Performance metrics for the vulnerability classification task.}
    \label{fig:classification_outcomes}
\end{figure}

\medskip


These results highlight that the system maintains very low false-negative rates across all configurations. When the \gls{jcg} is available, only 5~\% of vulnerabilities are missed, and even without it the rate remains at just 3~\%. 

At the same time, the absence of Java-side context increases uncertainty. When the \gls{jcg} is unavailable, the model compensates by adopting a more conservative stance, inflating the false-positive rate from 18~\% to 40~\%. This confirms that cross-layer information plays a critical role in helping the \gls{llm} distinguish between a vulnerability and a benign application-level fault.

Considering the full dataset, the aggregated results show a distribution of 17~\% true positives, 49~\% true negatives, 30~\% false positives, and only 4~\% false negatives. This overall pattern reinforces the behaviour observed in both experimental settings: the system reliably identifies most real vulnerabilities but tends to over-report suspicious cases when contextual information is limited. Such characteristics make the approach well suited for an initial triage stage. %, where minimising missed vulnerabilities is more critical than achieving perfect precision.



\begin{comment}
    
When the call graph is available, the system correctly identifies 21~\% of vulnerabilities (TP) and misses the 5~\% (FN), while producing 18~\% false alarms (FP) and 56~\% correct rejections (TN).  
Without the call graph, the model still detects 13~\% vulnerabilities and misses 3~\%, but generates a higher number of false positives 40~\%, with 44~\% true negatives.

Considering the dataset as a whole, the system reports 17~\% true positives and 49~\% true negatives, against 30~\% false positives and 4~\% false negatives.  
This distribution shows a clear tendency towards recall: the model successfully captures most real vulnerabilities, but at the cost of an increased number of false alarms, particularly when Java-side context is absent.
\end{comment}


\begin{table}[ht]
\centering
\begin{tabular}{|p{2cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
\hline 
\centering \textbf{Metric} 
& \centering \textbf{With \glsxtrshort{jcg}} 
& \centering \textbf{Without \glsxtrshort{jcg}} 
& \centering \textbf{All APKs} \tabularnewline
\hline
\hline

\centering \textbf{Accuracy}  
& \centering 77.05~\% 
& \centering 57.33~\% 
& \centering 66.18~\% \tabularnewline

\centering \textbf{Precision}    
& \centering 54.17~\% 
& \centering 25.00~\% 
& \centering 35.94~\% \tabularnewline

\centering \textbf{Recall} 
& \centering 81.25~\% 
& \centering 83.33~\% 
& \centering 82.14~\% \tabularnewline

\centering \textbf{F1-Score}  
& \centering 65.00~\% 
& \centering 38.46~\% 
& \centering 50.00~\% \tabularnewline
\hline
\end{tabular}

\caption{Evaluation metrics computed from the classification results.}
\label{tab:eval_metrics}
\end{table}

Table~\ref{tab:eval_metrics} summarises the overall performance of the system under the two conditions.  
The aggregated accuracy (66.18~\%) indicates that roughly two-thirds of all classifications match the ground truth.

Accuracy is substantially higher when the \glsxtrlong{jcg} is available (77.05~\%) than when it is not (57.33~\%), showing that Java-to-native context improves the model’s ability to correctly classify both vulnerable and non-vulnerable cases.

Precision shows an even more pronounced dependency on the availability of the \glsxtrshort{jcg}.  
With \gls{jcg}, 54.17~\% of positive predictions correspond to true vulnerabilities, whereas without it precision drops to 25.00~\%.  
Thus, the \gls{jcg} significantly reduces over-prediction and makes positive classifications more trustworthy.

Recall remains consistently high across both settings.  
With \gls{jcg}, the system correctly identifies 81.25~\% of all true vulnerabilities, and even without it recall stays at 83.33~\%.  
This shows that the system rarely misses dangerous cases, regardless of the context provided.

The F1-score highlights the overall effect of contextual information on the balance between precision and recall.  
With \gls{jcg}, the system reaches 65.00~\%, while without it the score falls to 38.46~\%.  
The combined F1-score across all crashes is 50.00~\%, indicating that the classifier is considerably more stable and better calibrated when Java-level execution context is available, and becomes less reliable when this information is removed.


\begin{comment}
    
Precision also benefits from the additional Java-side context.  
In the ``With \glsxtrshort{jcg}'' configuration, 56.25~\% of the crashes flagged as vulnerabilities correspond to true positives, whereas in the ``Without \glsxtrshort{jcg}'' case precision drops to 35.71~\%.  
This indicates that the \gls{jcg} not only increases the fraction of detected vulnerabilities, but also makes positive predictions more trustworthy.  

Recall is consistently high in both settings, but still shows a noticeable gap.  
With \glsxtrshort{jcg}, the system correctly recovers 81.82~\% of all true vulnerabilities, while without it recall drops to 71.43~\%.  
This confirms that providing the \gls{llm} with an explicit \gls{jcg} avoid missing dangerous cases.  

The F1-score, which combines precision and recall into a single metric, highlights the overall impact of the call graph.  
With \glsxtrshort{jcg}, the system reaches 66.67~\%, whereas without it the score falls to 47.62~\%.  
The combined F1-score across all crashes is 58.33~\%, indicating that, on average, the current implementation performs substantially better when \gls{jcg} information is available, and becomes less balanced and less reliable when this context is removed.
\end{comment}