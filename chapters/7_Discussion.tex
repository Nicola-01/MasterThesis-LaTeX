\chapter{Discussion}
\label{chp:disc}

\section{Tool Performance and Efficiency}

The implemented pipeline demonstrates acceptable and promising operational performance.  

\textit{Single-crash} analysis by the \gls{llm} requires on average less than a minute, which makes the approach scalable for datasets of moderate size.  
However, the total execution time per application depends heavily on fuzzer-generated crash count, the complexity of native stack traces, and the number of native libraries involved.  
In applications with many or large native libraries, the overhead caused by importing and decompiling binaries can substantially increase the total runtime, this extra cost is due to the reverse-engineering tools, not the analysis itself.

\section{Effect of \glsxtrlong{jcg}}

The comparison between analyses performed with or without the \gls{jcg} yields a clear trade-off:
\begin{myitemize}
    \item When the \glsxtrlong{jcg} is available (“With \glsxtrshort{jcg}”), the model can leverage both native and Java-level context, producing more coherent and realistic vulnerability assessments. Under this configuration, the system achieves higher recall and maintains relatively few false negatives.
    \item Without the \glsxtrlong{jcg} (“Without \glsxtrshort{jcg}”), the lack of Java-side context leads the model to over-approximate risks. The model tends to treat all inputs reaching native code as potentially attacker-controlled, resulting in a significantly higher number of false positives.
\end{myitemize}

This confirms the hypothesis that integrating cross-layer context (Java + native) is essential to reduce spurious vulnerability reports while preserving the capacity to detect real issues.

\section{Reliability}

A major strength of the pipeline is the low number of False Negatives: most actual vulnerabilities (i.e., real exploitable crashes) are correctly flagged by the system. In security auditing, such conservativeness—preferring to err on the side of caution—is often desirable.

This behaviour can be explained by the LLM’s tendency to classify ambiguous cases as vulnerabilities, particularly those involving memory-related failures such as buffer errors, JNI misuse, or improper native calls. This pattern is consistent with what has been observed in prior research on LLM-based vulnerability detection, including the work of Akuthota et al.~\cite{10456393}, which reports a marked inclination of GPT-based models to over-predict positives and thus generate a substantial number of false positives.

While this over-cautious behaviour increases the false-positive rate, it ensures that most true issues are reliably surfaced and can subsequently be triaged manually or processed by more precise, specialized analysis tools.

\begin{comment}
    
A major strength of the pipeline is the low number of False Negatives: most actual vulnerabilities (i.e.\ real exploitable crashes) are correctly flagged by the system.  
In security auditing, such conservativeness, preferring to err on the side of caution, is often desirable.  
The behaviour may be explained by the \gls{llm}’s bias toward flagging memory-related crashes (buffer errors, JNI misuse, improper native calls) as vulnerabilities whenever evidence is ambiguous — a pattern already documented in literature as a common issue of LLM-based vulnerability detection~\cite{10456393}.
Therefore, while this leads to many false positives, the system reliably surfaces most true issues, which can then be manually triaged or triaged by more precise tools.
\end{comment}

\section{Threats to Validity}

Several factors may compromise the validity of the results:

- \textbf{Dependence on crash trace quality.} The classification is highly dependent on the completeness and correctness of the stack traces generated by the fuzzer. In many cases, the trace contains ambiguous frames (e.g.\ marked as “\texttt{??}”) or omits intermediate functions, reducing the context available for analysis. This frequently causes uncertainty in classification and likely contributes to the high false-positive rate.

- \textbf{Intrinsic limitations of LLM reasoning.} Multiple recent studies highlight that LLMs may not reliably reason about code in security-sensitive contexts. They can hallucinate tool calls, misunderstand code semantics, or fail when minor modifications (function renaming, formatting changes) are applied — leading to inconsistent or incorrect vulnerability assessments. :contentReference[oaicite:2]{index=2}  
  This unpredictability and fragility limit the trustworthiness of purely automated triage, especially in complex or obfuscated binaries.

- \textbf{Dataset and sample bias.} The set of applications and crashes used in this evaluation may not cover all relevant real-world scenarios. For example, APKs with uncommon JNI patterns or exotic library structures might behave differently, thus the results may not generalize.

- \textbf{Lack of ground truth completeness.} The classification labels (vulnerable / non-vulnerable) hinge on manual or heuristic validation of crash exploitability, which may miss subtle vulnerabilities or mislabel benign crashes — introducing noise in the evaluation metrics.

\section{Implications and Future Directions}

The results suggest that LLM-based triage, when enriched with binary and bytecode context via MCP tools, can serve as an effective \emph{preliminary filter} in a larger vulnerability analysis pipeline:  
it reduces analysts’ workload by quickly flagging likely vulnerabilities, while deferring manual verification to a later stage.

Nevertheless, improving precision — reducing false positives — remains essential. Possible future improvements include:

\begin{itemize}
  \item improving the quality and completeness of fuzzer-generated stack traces;  
  \item integrating additional static or dynamic analyses (e.g.\ symbolic execution, taint tracking) to corroborate or refute the LLM’s conclusions;  
  \item augmenting context supplied to the model (e.g.\ via RAG, code summary, better call-graph reconstruction) to reduce uncertainty;  
  \item exploring persistent agent architectures or repeated analysis with multiple LLMs to increase robustness and consensus.  
\end{itemize}

\section{Summary}

In conclusion, the proposed LLM + MCP pipeline achieves a favorable balance between automation, recall, and analysis speed.  
It excels at surfacing real vulnerabilities (low false negatives) while maintaining relatively low computational cost per crash.  
However, the significant false-positive rate, combined with dependencies on evidence quality and LLM limitations, indicates that such a system is best suited as an initial triage filter — not a final decision tool.  
Future work should focus on improving precision, robustness, and coverage.
