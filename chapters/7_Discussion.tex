\chapter{Discussion}
\label{chp:disc}

\section{Tool Performance and Efficiency}

The implemented pipeline demonstrates acceptable and promising operational performance.  


\textit{Single-crash} analysis by the \gls{llm} requires on average less than a minute, which makes the approach scalable and significantly accelerates the classification process.
However, the total execution time per application depends heavily on fuzzing procedure, the crash count of method, the complexity of native stack traces, and the number of native libraries involved.  
In applications with many or large native libraries, the overhead caused by importing and decompiling binaries can substantially increase the total runtime, this overhead originates from the reverse-engineering tools rather than from the analysis performed by the model.

\section{Effect of \glsxtrlong{jcg}}

As seen in Chapter~\ref{chp:result}, the comparison between analyses performed with or without the \gls{jcg} yields a clear trade-off:

\begin{myitemize}
    \item When the \glsxtrlong{jcg} is available (“With \glsxtrshort{jcg}”), the model can leverage both native and Java-level context, producing more coherent and realistic vulnerability assessments. Under this configuration, the system achieves higher recall and maintains relatively few false negatives.
    \item Without the \glsxtrlong{jcg} (“Without \glsxtrshort{jcg}”), the lack of Java-side context leads the model to over-approximate risks. Without knowing the origin or constraints of the data, the system tends to treat values as attacker-controlled, even when in reality they may originate from fixed Java-side logic and be impossible for an attacker to influence.
\end{myitemize}


\section{Reliability}

The pipeline exhibits a low number of False Negatives, indicating that genuine vulnerabilities are rarely overlooked. This behaviour is partly influenced by the \gls{llm}'s tendency to classify ambiguous memory-related faults (e.g., buffer errors, JNI misuse, invalid native calls) as potentially dangerous. Prior studies highlight similar patterns: general-purpose models often over-predict positives, hallucinate tool behaviour, or misinterpret control-flow semantics~\cite{10456393,ullah2024llmsreliablyidentifyreason}. %Such limitations make them conservative but also fragile, reinforcing the need to complement their output with more deterministic analyses rather than relying on them as stand-alone security oracles.


%There are several factors limit the reliability and generalisability of the results.

\section{Limitations}

The evaluation performed in this thesis is subject to several limitations that affect the results.

The quality of the classification strongly depends entirely on the crash traces produced by POIROT. When stack trace contain missing or unresolved entries (marked as ``\texttt{??}''), the model lacks essential execution context and tends to assume a worst-case scenario, contributing to the high false-positive rate observed in the evaluation.

The dataset, though representative of diverse applications, cannot fully capture the variability of real-world Android ecosystems. %In practice, the Android landscape includes a wide range of device vendors, customised firmware, architecture-specific native libraries, and highly heterogeneous JNI integration patterns. Such diversity may expose corner cases or interaction behaviours that are not reflected in the evaluated sample, limiting the extent to which the observed results can be generalised.


%Additionally, the inherent limitations of current LLMs in code reasoning introduce further uncertainty. Recent studies show that these models may hallucinate tool behaviour, misinterpret control-flow semantics, or become unstable under minor program transformations~\cite{ullah2024llmsreliablyidentifyreason}. Such fragility makes them unsuitable as stand-alone security oracles and suggests that their output should be complemented with deterministic analyses.

%Another limiting factor concerns the ground truth used for evaluation. Labels indicating whether a crash is truly exploitable rely on heuristic validation and may themselves contain inaccuracies. Moreover, the dataset—though representative of diverse applications—cannot fully capture the variability of real-world Android ecosystems.

%In summary, while the system reliably surfaces true vulnerabilities, its precision is constrained by fuzzer trace quality, LLM reasoning limitations, and incomplete ground truth, all of which must be considered when interpreting the results.
